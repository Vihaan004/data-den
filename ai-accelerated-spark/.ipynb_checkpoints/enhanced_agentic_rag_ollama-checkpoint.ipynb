{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f625337",
   "metadata": {},
   "source": [
    "# Agentic RAG with Local Ollama Model\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) agent using LangGraph, LangChain, and a local  model run via Ollama.\n",
    "\n",
    "Adapted from: https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/\n",
    "\n",
    "## Materials\n",
    "This notebook and all materials referenced here can be found on Sol `/data/sse/ai-accelerated-spark`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc8378-b454-48d8-a95c-563521334562",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13cfaace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 19:48:24.076648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750906104.091197 1638104 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750906104.095614 1638104 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750906104.109124 1638104 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750906104.109139 1638104 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750906104.109141 1638104 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750906104.109142 1638104 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-25 19:48:24.114023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.tools import Tool\n",
    "from langgraph.graph import Graph\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59114acf",
   "metadata": {},
   "source": [
    "## 2. Preprocess documents\n",
    "### 2.1. Fetch documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24bf7599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://medium.com/cupy-team/announcing-cupy-v13-66979ee7fab0\",\n",
    "    \"https://www.unum.cloud/blog/2022-01-26-cupy\",\n",
    "    \"https://medium.com/rapids-ai/easy-cpu-gpu-arrays-and-dataframes-run-your-dask-code-where-youd-like-e349d92351d\"\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d6bc96c-c2c6-4ca8-af17-1a0dc06d46cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Announcing CuPy v13. We are happy to announce that CuPy v13… | by Kenichi Maehashi | CuPy | MediumSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inCuPy·NumPy & SciPy for GPU − News from the CuPy TeamAnnouncing CuPy v13Kenichi MaehashiFollow3 min read·Jan 18, 2024--ListenShareWe are happy to announce that CuPy v13 is now available. This new major release contains the effort of over 270 pull requests, including more SciPy-compatible routines and better packaging. Let’s check out the highlights:Signal Processing APIs − `cupyx.scipy.signal`In this release, we have added 140 signal processing routines that are compatible with SciPy’s scipy.signal.* APIs. The API Reference lists all the available signal functions. This work has been done by Evgani Burovski and Edgar Andrés Margffoy Tuay from Quansight, under the support of CZI’s Essential OSS for Science program.We would also like to highlight cuSignal’s contributions in this release. cuSignal is a library developed by the NVIDI'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0][0].page_content.strip()[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dcb754-10f3-4a16-b522-c1ec4cbd812b",
   "metadata": {},
   "source": [
    "### 2.2. Split the fetched documents into smaller chunks for indexing into the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97649da8-f2bc-49fd-be94-009b59a1a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15cad859-5123-4811-b53a-a58ee9be9232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Announcing CuPy v13. We are happy to announce that CuPy v13… | by Kenichi Maehashi | CuPy | MediumSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inCuPy·NumPy & SciPy for GPU − News from the CuPy TeamAnnouncing CuPy v13Kenichi MaehashiFollow3 min read·Jan 18, 2024--ListenShareWe are happy to announce that CuPy v13 is now'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_splits[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbf0613",
   "metadata": {},
   "source": [
    "## 3.Create a retriever tool\n",
    "### 3.1. Use an in-memory vector store and all-MiniLM-L6-V2 embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edb39231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents = doc_splits, embedding = embedding_model\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abcb57e5-dcb5-4387-8327-5756c2df4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use ChromaDB for persistent vectorstore\n",
    "# https://python.langchain.com/docs/integrations/vectorstores/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f517c44-2e14-4614-9290-8d75e7faef4f",
   "metadata": {},
   "source": [
    "### 3.2. Create a retriever tool using LangChain's prebuild `create_retriever_tool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5278231-c5eb-40ca-94f5-8a74d99a1b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_python_gpu_acceleration\",\n",
    "    \"Search and return information about accelerating Python code using the GPU with RAPIDS and CuPy.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fcbee6-e9d6-466f-9c7a-303ca012b469",
   "metadata": {},
   "source": [
    "### 3.3. Test the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ba4b915-9cfc-4e71-9ea2-6060cde12d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chunksize=(2, 5), \\\\chunktype=cupy.ndarray>Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with dask.config.set({“array.backend”: “cupy”}):…    darr = da.random.randint(0, 3, size=(10, 20), chunks=(2, 5)) #\\n\\n= rs.randint(0, 3, size=(10, 20), chunks=(2, 5))>>> darrdask.array<randint, shape=(10, 20), dtype=int64, chunksize=(2, 5), \\\\chunktype=cupy.ndarray>Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with\\n\\nfor random array creation. Previously, if a user wanted to create a CuPy-backed Dask array, they were required to define an explicit RandomState object in Dask using CuPy. For example, the following code worked prior to Dask 2022.10.0, but seems rather verbose:>>> import cupy>>> import dask.array as da>>> rs = da.random.RandomState(RandomState=cupy.random.RandomState)>>>>>> darr =\\n\\n\\\\chunktype=cupy.ndarray>The chunktype informs us that the array is constructed with cupy.ndarrayobjects instead of numpy.ndarray objects.We’ve also improved the user experience for random array creation. Previously, if a user wanted to create a CuPy-backed Dask array, they were required to define an explicit RandomState object in Dask using CuPy. For example, the following code worked prior to Dask'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool.invoke({\"query\": \"How can I create a CuPy-backed Dask array for random data?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29127ddb-5909-4809-8525-3204ef2d08cf",
   "metadata": {},
   "source": [
    "## 4. Generate query\n",
    "### 4.1. Load local LLM\n",
    "\n",
    "Start ollama using the terminal:\n",
    "```bash\n",
    "module load ollama/0.9.0\n",
    "export OLLAMA_MODELS=/data/datasets/community/ollama\n",
    "ollama-start\n",
    "```\n",
    "\n",
    "Check the available list of models using `ollama list`. Let me know via Slack if you would like to use and test other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4504022-deee-4190-b681-6dfa9ba5e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import socket\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "host_node = socket.gethostname()\n",
    "llm_model = init_chat_model(\"ollama:qwen3:14b\", temperature=0, base_url=f\"http://jgarc111@{host_node}:11434/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972da38e-7439-4d43-8fc6-6aafcdf00069",
   "metadata": {},
   "source": [
    "### 4.2. Build a `generate_query_or_respond` node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65ecc05d-868d-4228-bc67-bd8ed7b65a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "import re\n",
    "\n",
    "def generate_query_or_respond(state: MessagesState):\n",
    "    \"\"\"Call the model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n",
    "    \"\"\"\n",
    "    response = (\n",
    "        llm_model\n",
    "        .bind_tools([retriever_tool]).invoke(state[\"messages\"])\n",
    "    )\n",
    "    # remove thinking text\n",
    "    content = re.sub(r\"<think>.*</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
    "    response.content = content\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0091934-a78a-4a46-bdd6-24d62eae69ed",
   "metadata": {},
   "source": [
    "### 4.3. Try a random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fbc5702-ff03-4934-b664-3d6a8edca463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The function provided is for retrieving information about accelerating Python code using GPUs, which is unrelated to the color of the sky. I cannot answer this question with the available tools. Please ask a question related to Python GPU acceleration with RAPIDS or CuPy.\n"
     ]
    }
   ],
   "source": [
    "input = {\"messages\": [{\"role\": \"user\", \"content\": \"Hello! What is the color of the sky?\"}]}\n",
    "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50e1018-8b5c-405e-ab80-2bfc79e9a7ab",
   "metadata": {},
   "source": [
    "### 4.4. Try semantic search question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "934dd1bc-fa2c-414c-80f9-cb94b93ffde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_python_gpu_acceleration (e1e7d32e-6f25-483f-80f9-de75479f4025)\n",
      " Call ID: e1e7d32e-6f25-483f-80f9-de75479f4025\n",
      "  Args:\n",
      "    query: creating CuPy-backed Dask arrays for random data\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddaf2a9-6dac-4e24-a77d-2ee588ac6eed",
   "metadata": {},
   "source": [
    "## 5. Grade documents\n",
    "### 5.1. Add conditional edge `grade_documents` to determine the relevance of retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6294ac9-5927-4f26-ac89-046829bcdfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "GRADE_PROMPT = (\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n",
    "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
    "    \"Here is the user question: {question} \\n\"\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
    ")\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n",
    "    )\n",
    "\n",
    "\n",
    "def grade_documents(\n",
    "    state: MessagesState,\n",
    ") -> Literal[\"generate_answer\", \"rewrite_question\"]:\n",
    "    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    \n",
    "\n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = (\n",
    "        llm_model\n",
    "        .with_structured_output(GradeDocuments).invoke(\n",
    "            [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    )\n",
    "    score = response.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        return \"rewrite_question\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a918ef9-e933-4f0c-83ce-75d02585a241",
   "metadata": {},
   "source": [
    "### 5.2. Try with irrelevant documents in the tool response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00be481b-9484-40db-98c5-f227129cafd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rewrite_question'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
    "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4e35d-63e6-4658-9ade-c6e835c6e39e",
   "metadata": {},
   "source": [
    "### 5.3. Try with relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d7119fa-55d5-4179-8bcc-661a68e100b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generate_answer'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
    "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": 'Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with dask.config.set({“array.backend”: “cupy”}):…    darr = da.random.randint(0, 3, size=(10, 20), chunks=(2, 5)) #\\n\\n= rs.randint(0, 3, size=(10, 20), chunks=(2, 5))>>> darrdask.array<randint, shape=(10, 20), dtype=int64, chunksize=(2, 5), \\\\chunktype=cupy.ndarray>Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with\\n\\nfor random array creation.',\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6520541-4db0-4db2-839d-66d74494644b",
   "metadata": {},
   "source": [
    "## 6.\n",
    "### 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "199cb600-4cad-4e9a-80fc-93746877b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_PROMPT = (\n",
    "    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n",
    "    \"Here is the initial question:\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"Formulate an improved question:\"\n",
    ")\n",
    "\n",
    "\n",
    "def rewrite_question(state: MessagesState):\n",
    "    \"\"\"Rewrite the original user question.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = llm_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    # remove thinking text\n",
    "    content = re.sub(r\"<think>.*</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
    "    response.content = content\n",
    "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025fedf-4e70-443f-acae-c384b7163d8f",
   "metadata": {},
   "source": [
    "### 6.2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "114f4730-0d74-4172-aaf8-9dc8c55a60a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Improved Question:**  \n",
      "How can I generate a large random dataset using CuPy's GPU acceleration and integrate it with Dask arrays for parallel processing, including the necessary steps to ensure compatibility between CuPy and Dask, optimal chunk sizing, and efficient memory management?  \n",
      "\n",
      "**Rationale for Improvement:**  \n",
      "The revised question explicitly addresses:  \n",
      "1. **Integration of CuPy and Dask** (clarifying the need for compatibility checks).  \n",
      "2. **Use case** (generating large random data for parallel processing).  \n",
      "3. **Implementation details** (chunk sizing, memory management, and best practices).  \n",
      "4. **Underlying intent** (leveraging GPU acceleration with Dask for scalability).  \n",
      "This avoids ambiguity and guides the responder to address both technical steps and performance considerations.\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
    "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = rewrite_question(input)\n",
    "print(response[\"messages\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0771d-200e-4a1c-9da3-e8a4e82fb480",
   "metadata": {},
   "source": [
    "## 7. Generate an answer\n",
    "### 7.1. Build `generate_answer` node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3472875-cfd6-4b3f-8b39-7a1139bd7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Question: {question} \\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_answer(state: MessagesState):\n",
    "    \"\"\"Generate an answer.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = llm_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    # remove thinking text\n",
    "    content = re.sub(r\"<think>.*</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
    "    response.content = content\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd1250-820e-40f8-ad58-c85bca8e2c2e",
   "metadata": {},
   "source": [
    "## 7.2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76d74558-2403-4c2c-a50c-944cd63dc501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To create a CuPy-backed Dask array for random data, set the `array.backend` configuration to `\"cupy\"` using `dask.config.set`. Use `da.random.randint` with desired parameters, such as `size=(10, 20)` and `chunks=(2, 5)`. This generates a Dask array with CuPy as the underlying backend.\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
    "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": 'Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with dask.config.set({“array.backend”: “cupy”}):…    darr = da.random.randint(0, 3, size=(10, 20), chunks=(2, 5)) #\\n\\n= rs.randint(0, 3, size=(10, 20), chunks=(2, 5))>>> darrdask.array<randint, shape=(10, 20), dtype=int64, chunksize=(2, 5), \\\\chunktype=cupy.ndarray>Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with\\n\\nfor random array creation.',\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = generate_answer(input)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb5fc5f-b09c-42b1-88d9-f2b6784b2eda",
   "metadata": {},
   "source": [
    "## 8. Assemble the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49bf2038-5fa5-4e22-a208-4b1167e1cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(generate_query_or_respond)\n",
    "workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
    "workflow.add_node(rewrite_question)\n",
    "workflow.add_node(generate_answer)\n",
    "\n",
    "workflow.add_edge(START, \"generate_query_or_respond\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_query_or_respond\",\n",
    "    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d44857e0-4ce3-4a02-ba3b-ade019288936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc9c8a-4a28-49dc-ae10-ca7b8eb42e43",
   "metadata": {},
   "source": [
    "## 9. Run the agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90d926a1-4f35-42ed-bd08-1027c975644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node generate_query_or_respond\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_python_gpu_acceleration (983d9a98-8938-4dbd-8611-7946f30d8de7)\n",
      " Call ID: 983d9a98-8938-4dbd-8611-7946f30d8de7\n",
      "  Args:\n",
      "    query: creating CuPy-backed Dask arrays for random data\n",
      "\n",
      "\n",
      "\n",
      "Update from node retrieve\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_python_gpu_acceleration\n",
      "\n",
      "= rs.randint(0, 3, size=(10, 20), chunks=(2, 5))>>> darrdask.array<randint, shape=(10, 20), dtype=int64, chunksize=(2, 5), \\chunktype=cupy.ndarray>Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with\n",
      "\n",
      "chunksize=(2, 5), \\chunktype=cupy.ndarray>Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with dask.config.set({“array.backend”: “cupy”}):…    darr = da.random.randint(0, 3, size=(10, 20), chunks=(2, 5)) #\n",
      "\n",
      "for random array creation. Previously, if a user wanted to create a CuPy-backed Dask array, they were required to define an explicit RandomState object in Dask using CuPy. For example, the following code worked prior to Dask 2022.10.0, but seems rather verbose:>>> import cupy>>> import dask.array as da>>> rs = da.random.RandomState(RandomState=cupy.random.RandomState)>>>>>> darr =\n",
      "\n",
      "\\chunktype=cupy.ndarray>The chunktype informs us that the array is constructed with cupy.ndarrayobjects instead of numpy.ndarray objects.We’ve also improved the user experience for random array creation. Previously, if a user wanted to create a CuPy-backed Dask array, they were required to define an explicit RandomState object in Dask using CuPy. For example, the following code worked prior to Dask\n",
      "\n",
      "\n",
      "\n",
      "Update from node generate_answer\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To create a CuPy-backed Dask array for random data, set the `array.backend` configuration to `\"cupy\"` using `dask.config.set`. Then use `dask.array.random.randint` with desired parameters, such as `da.random.randint(0, 3, size=(10, 20), chunks=(2, 5))`. This automatically creates arrays with `cupy.ndarray` chunks instead of `numpy.ndarray`.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    for node, update in chunk.items():\n",
    "        print(\"Update from node\", node)\n",
    "        update[\"messages\"][-1].pretty_print()\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131337e-17ea-49e6-a9aa-b0c4798ddfe1",
   "metadata": {},
   "source": [
    "## 10. Graphic User Interface using Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d135bb8-88ae-4cc1-9a5a-5ffde1232826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://f815a2234e1f77ed53.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f815a2234e1f77ed53.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def ask_graph(user_input, chat_history):\n",
    "    result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    response = result[\"messages\"][-1].content\n",
    "\n",
    "    if not chat_history:\n",
    "        response = [{\"role\": \"user\", \"content\": user_input}, {\"role\": \"assistant\", \"content\": response}]\n",
    "    else:\n",
    "        response = chat_history + [{\"role\": \"user\", \"content\": user_input}, {\"role\": \"assistant\", \"content\": response}]\n",
    "\n",
    "    return \"\", response\n",
    "\n",
    "def clear_conversation():\n",
    "    return \"\", \"\"\n",
    "\n",
    "with gr.Blocks(fill_height=True, fill_width=True) as demo:\n",
    "    gr.Markdown(\"### Agentic RAG\")\n",
    "\n",
    "    with gr.Column():\n",
    "\n",
    "        with gr.Row():\n",
    "            chatbot = gr.Chatbot(height=350, type=\"messages\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=4):\n",
    "                query_input = gr.Textbox(\n",
    "                    label=\"Enter text here\", placeholder=\"Ask something...\", lines=1\n",
    "                    )\n",
    "            with gr.Column(scale=1):\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"⬆\")\n",
    "                # 🧹 Clear button\n",
    "                with gr.Row():\n",
    "                    clear_btn = gr.Button(\"🧹 Clear Conversation\")\n",
    "\n",
    "        submit_btn.click(\n",
    "            fn=ask_graph,\n",
    "            inputs=[query_input, chatbot],\n",
    "            outputs=[query_input, chatbot],\n",
    "        )\n",
    "\n",
    "        query_input.submit(\n",
    "            fn=ask_graph,\n",
    "            inputs=[query_input, chatbot],\n",
    "            outputs=[query_input, chatbot],\n",
    "        )\n",
    "\n",
    "        clear_btn.click(\n",
    "            fn=clear_conversation,\n",
    "            outputs=[query_input, chatbot],\n",
    "        )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fe47fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ plotly - Available\n",
      "✅ gradio - Available\n",
      "✅ pandas - Available\n",
      "✅ numpy - Available\n",
      "\n",
      "🎉 All required packages are available!\n",
      "\n",
      "--- Sol-specific checks ---\n",
      "✅ subprocess - Available\n",
      "✅ uuid - Available\n",
      "✅ pathlib - Available\n",
      "✅ json - Available\n",
      "✅ tempfile - Available\n"
     ]
    }
   ],
   "source": [
    "# Check dependencies for Enhanced GPU Mentor\n",
    "import sys\n",
    "\n",
    "required_packages = {\n",
    "    'plotly': 'plotly',\n",
    "    'gradio': 'gradio', \n",
    "    'pandas': 'pandas',\n",
    "    'numpy': 'numpy'\n",
    "}\n",
    "\n",
    "missing_packages = []\n",
    "\n",
    "for package, import_name in required_packages.items():\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"✅ {package} - Available\")\n",
    "    except ImportError:\n",
    "        print(f\"❌ {package} - Missing\")\n",
    "        missing_packages.append(package)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\nInstall missing packages with:\")\n",
    "    print(f\"pip install {' '.join(missing_packages)}\")\n",
    "else:\n",
    "    print(\"\\n🎉 All required packages are available!\")\n",
    "\n",
    "# Check Sol-specific modules (these should be available when running on Sol)\n",
    "print(\"\\n--- Sol-specific checks ---\")\n",
    "sol_modules = ['subprocess', 'uuid', 'pathlib', 'json', 'tempfile']\n",
    "for module in sol_modules:\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"✅ {module} - Available\")\n",
    "    except ImportError:\n",
    "        print(f\"❌ {module} - Missing (this should not happen)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591264d3",
   "metadata": {},
   "source": [
    "# GPU Mentor: Enhanced RAG with Code Execution & Benchmarking\n",
    "\n",
    "This enhanced version of the Agentic RAG system includes:\n",
    "- **Code Execution on Sol**: Submit and execute user code on Sol's GPU nodes\n",
    "- **Performance Benchmarking**: Compare CPU vs GPU performance with RAPIDS libraries\n",
    "- **Code Optimization**: Automatically suggest GPU-accelerated alternatives\n",
    "- **Interactive Learning**: Socratic questioning to guide learning\n",
    "\n",
    "## Architecture Overview\n",
    "1. **RAG Agent**: Existing system for answering questions about GPU acceleration\n",
    "2. **Code Executor**: Submits jobs to Sol via SLURM\n",
    "3. **Benchmark Engine**: Measures and compares CPU/GPU performance\n",
    "4. **Code Optimizer**: Suggests RAPIDS/CuPy alternatives\n",
    "5. **Enhanced UI**: Comprehensive interface for code playground and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40018daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced imports for GPU Mentor\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import ast\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6dca4",
   "metadata": {},
   "source": [
    "## 11. Sol Code Executor - SLURM Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86b85430",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolCodeExecutor:\n",
    "    \"\"\"\n",
    "    Executes code on Sol supercomputer via SLURM job submission.\n",
    "    Handles both CPU and GPU benchmarking jobs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_work_dir=\"/tmp/gpu_mentor\"):\n",
    "        self.base_work_dir = Path(base_work_dir)\n",
    "        self.base_work_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def create_slurm_script(self, code: str, job_type: str = \"cpu\", \n",
    "                           time_limit: str = \"00:15:00\", \n",
    "                           memory: str = \"32G\") -> str:\n",
    "        \"\"\"Create SLURM batch script for code execution.\"\"\"\n",
    "        \n",
    "        job_id = str(uuid.uuid4())[:8]\n",
    "        script_content = \"\"\n",
    "        \n",
    "        if job_type == \"cpu\":\n",
    "            script_content = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=gpu_mentor_cpu_{job_id}\n",
    "#SBATCH --partition=general\n",
    "#SBATCH --qos=public\n",
    "#SBATCH --time={time_limit}\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --mem={memory}\n",
    "#SBATCH --output=cpu_output_{job_id}.out\n",
    "#SBATCH --error=cpu_error_{job_id}.err\n",
    "\n",
    "# Load necessary modules\n",
    "module load python/3.11\n",
    "module load anaconda3\n",
    "\n",
    "# Activate conda environment with CPU libraries\n",
    "source activate base\n",
    "\n",
    "# Create timing wrapper\n",
    "cat > benchmark_cpu_{job_id}.py << 'SCRIPT_EOF'\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "try:\n",
    "{self._indent_code(code)}\n",
    "    execution_status = \"success\"\n",
    "    error_message = \"\"\n",
    "except Exception as e:\n",
    "    execution_status = \"error\"\n",
    "    error_message = str(e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Save benchmark results\n",
    "results = {{\n",
    "    \"execution_time\": execution_time,\n",
    "    \"job_type\": \"cpu\",\n",
    "    \"job_id\": \"{job_id}\",\n",
    "    \"status\": execution_status,\n",
    "    \"error\": error_message\n",
    "}}\n",
    "\n",
    "with open(\"cpu_benchmark_{job_id}.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(f\"CPU Execution time: {{execution_time:.4f}} seconds\")\n",
    "SCRIPT_EOF\n",
    "\n",
    "# Execute the benchmark script\n",
    "python benchmark_cpu_{job_id}.py\n",
    "\"\"\"\n",
    "        else:  # GPU job\n",
    "            script_content = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=gpu_mentor_gpu_{job_id}\n",
    "#SBATCH --partition=general\n",
    "#SBATCH --qos=public\n",
    "#SBATCH --time={time_limit}\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --mem={memory}\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --output=gpu_output_{job_id}.out\n",
    "#SBATCH --error=gpu_error_{job_id}.err\n",
    "\n",
    "# Load necessary modules\n",
    "module load python/3.11\n",
    "module load anaconda3\n",
    "module load cuda/12.1\n",
    "\n",
    "# Activate conda environment with GPU libraries\n",
    "source activate rapids-23.08\n",
    "\n",
    "# Create timing wrapper\n",
    "cat > benchmark_gpu_{job_id}.py << 'SCRIPT_EOF'\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "\n",
    "# Import GPU libraries\n",
    "try:\n",
    "    import cupy as cp\n",
    "    import cudf\n",
    "    import cuml\n",
    "    gpu_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"GPU libraries not available: {{e}}\")\n",
    "    gpu_available = False\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "try:\n",
    "{self._indent_code(code)}\n",
    "    execution_status = \"success\"\n",
    "    error_message = \"\"\n",
    "except Exception as e:\n",
    "    execution_status = \"error\"\n",
    "    error_message = str(e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Save benchmark results\n",
    "results = {{\n",
    "    \"execution_time\": execution_time,\n",
    "    \"job_type\": \"gpu\",\n",
    "    \"job_id\": \"{job_id}\",\n",
    "    \"status\": execution_status,\n",
    "    \"error\": error_message,\n",
    "    \"gpu_available\": gpu_available\n",
    "}}\n",
    "\n",
    "with open(\"gpu_benchmark_{job_id}.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(f\"GPU Execution time: {{execution_time:.4f}} seconds\")\n",
    "SCRIPT_EOF\n",
    "\n",
    "# Execute the benchmark script\n",
    "python benchmark_gpu_{job_id}.py\n",
    "\"\"\"\n",
    "        \n",
    "        return script_content, job_id\n",
    "    \n",
    "    def _indent_code(self, code: str, indent: str = \"    \") -> str:\n",
    "        \"\"\"Add proper indentation to user code for embedding in script.\"\"\"\n",
    "        return \"\\n\".join(indent + line for line in code.split(\"\\n\"))\n",
    "    \n",
    "    def submit_job(self, script_content: str, job_id: str) -> str:\n",
    "        \"\"\"Submit job to SLURM and return job ID.\"\"\"\n",
    "        script_path = self.base_work_dir / f\"job_{job_id}.sh\"\n",
    "        \n",
    "        with open(script_path, 'w') as f:\n",
    "            f.write(script_content)\n",
    "        \n",
    "        try:\n",
    "            # Submit job via sbatch\n",
    "            result = subprocess.run(\n",
    "                [\"sbatch\", str(script_path)],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                cwd=self.base_work_dir\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                # Extract SLURM job ID from output\n",
    "                slurm_job_id = result.stdout.strip().split()[-1]\n",
    "                return slurm_job_id\n",
    "            else:\n",
    "                raise Exception(f\"Job submission failed: {result.stderr}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error submitting job: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def check_job_status(self, slurm_job_id: str) -> str:\n",
    "        \"\"\"Check the status of a SLURM job.\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"squeue\", \"-j\", slurm_job_id, \"-h\", \"-o\", \"%T\"],\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0 and result.stdout.strip():\n",
    "                return result.stdout.strip()\n",
    "            else:\n",
    "                # Job might be completed, check sacct\n",
    "                result = subprocess.run(\n",
    "                    [\"sacct\", \"-j\", slurm_job_id, \"-n\", \"-o\", \"State\"],\n",
    "                    capture_output=True,\n",
    "                    text=True\n",
    "                )\n",
    "                if result.returncode == 0 and result.stdout.strip():\n",
    "                    return result.stdout.strip().split()[0]\n",
    "                else:\n",
    "                    return \"UNKNOWN\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking job status: {e}\")\n",
    "            return \"ERROR\"\n",
    "    \n",
    "    def get_job_results(self, job_id: str, job_type: str) -> Dict:\n",
    "        \"\"\"Retrieve benchmark results from completed job.\"\"\"\n",
    "        result_file = self.base_work_dir / f\"{job_type}_benchmark_{job_id}.json\"\n",
    "        \n",
    "        if result_file.exists():\n",
    "            with open(result_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            return {\"error\": \"Results file not found\"}\n",
    "    \n",
    "    def cleanup_job_files(self, job_id: str):\n",
    "        \"\"\"Clean up temporary job files.\"\"\"\n",
    "        patterns = [\n",
    "            f\"job_{job_id}.sh\",\n",
    "            f\"*_output_{job_id}.out\",\n",
    "            f\"*_error_{job_id}.err\",\n",
    "            f\"*_benchmark_{job_id}.py\",\n",
    "            f\"*_benchmark_{job_id}.json\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            for file_path in self.base_work_dir.glob(pattern):\n",
    "                try:\n",
    "                    file_path.unlink()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error cleaning up {file_path}: {e}\")\n",
    "\n",
    "# Initialize the Sol executor\n",
    "sol_executor = SolCodeExecutor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1440d",
   "metadata": {},
   "source": [
    "## 12. Code Optimizer - GPU Acceleration Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b7c9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeOptimizer:\n",
    "    \"\"\"\n",
    "    Analyzes user code and suggests GPU-accelerated alternatives using RAPIDS and CuPy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.optimization_patterns = {\n",
    "            # NumPy to CuPy optimizations\n",
    "            'numpy': {\n",
    "                'import numpy as np': 'import cupy as np',\n",
    "                'np.array(': 'cp.array(',\n",
    "                'np.random.': 'cp.random.',\n",
    "                'np.linalg.': 'cp.linalg.',\n",
    "                'np.fft.': 'cp.fft.',\n",
    "                '.cpu()': '',  # Remove .cpu() calls\n",
    "            },\n",
    "            \n",
    "            # Pandas to cuDF optimizations\n",
    "            'pandas': {\n",
    "                'import pandas as pd': 'import cudf as pd',\n",
    "                'pd.DataFrame(': 'cudf.DataFrame(',\n",
    "                'pd.Series(': 'cudf.Series(',\n",
    "                'pd.read_csv(': 'cudf.read_csv(',\n",
    "                'pd.read_parquet(': 'cudf.read_parquet(',\n",
    "                '.to_pandas()': '',  # Remove .to_pandas() calls\n",
    "            },\n",
    "            \n",
    "            # Scikit-learn to cuML optimizations\n",
    "            'sklearn': {\n",
    "                'from sklearn.': 'from cuml.',\n",
    "                'sklearn.': 'cuml.',\n",
    "            },\n",
    "            \n",
    "            # Dask optimizations\n",
    "            'dask': {\n",
    "                'import dask.array as da': 'import dask.array as da\\\\n# Configure Dask to use CuPy backend\\\\nimport dask\\\\ndask.config.set({\"array.backend\": \"cupy\"})',\n",
    "                'import dask.dataframe as dd': 'import dask_cudf as dd',\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_code(self, code: str) -> Dict[str, any]:\n",
    "        \"\"\"Analyze code for optimization opportunities.\"\"\"\n",
    "        analysis = {\n",
    "            'libraries_detected': [],\n",
    "            'optimization_opportunities': [],\n",
    "            'estimated_speedup': 1.0,\n",
    "            'gpu_compatible': True,\n",
    "            'warnings': []\n",
    "        }\n",
    "        \n",
    "        # Detect libraries used\n",
    "        for lib_type, patterns in self.optimization_patterns.items():\n",
    "            for pattern in patterns.keys():\n",
    "                if pattern in code:\n",
    "                    analysis['libraries_detected'].append(lib_type)\n",
    "                    break\n",
    "        \n",
    "        # Check for GPU incompatible operations\n",
    "        incompatible_patterns = [\n",
    "            'matplotlib.pyplot',  # Plotting might need CPU arrays\n",
    "            'pickle.dump',        # Serialization issues\n",
    "            'multiprocessing',    # GPU memory management conflicts\n",
    "        ]\n",
    "        \n",
    "        for pattern in incompatible_patterns:\n",
    "            if pattern in code:\n",
    "                analysis['warnings'].append(f\"Detected {pattern} - may require CPU data conversion\")\n",
    "        \n",
    "        # Estimate potential speedup based on operations\n",
    "        compute_intensive_ops = [\n",
    "            'np.dot', 'np.matmul', '@',  # Matrix operations\n",
    "            'np.fft', 'scipy.fft',       # FFT operations\n",
    "            '.groupby(', '.agg(',        # Aggregation operations\n",
    "            'for ' in code and 'range(' in code,  # Loops that could be vectorized\n",
    "        ]\n",
    "        \n",
    "        speedup_factors = []\n",
    "        for op in compute_intensive_ops:\n",
    "            if isinstance(op, bool):\n",
    "                if op:\n",
    "                    speedup_factors.append(5.0)  # Loop vectorization\n",
    "            elif op in code:\n",
    "                if 'matmul' in op or 'dot' in op or '@' in op:\n",
    "                    speedup_factors.append(10.0)  # Matrix ops\n",
    "                elif 'fft' in op:\n",
    "                    speedup_factors.append(15.0)  # FFT ops\n",
    "                else:\n",
    "                    speedup_factors.append(3.0)   # Other ops\n",
    "        \n",
    "        if speedup_factors:\n",
    "            analysis['estimated_speedup'] = max(speedup_factors)\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def suggest_optimizations(self, code: str) -> str:\n",
    "        \"\"\"Generate GPU-optimized version of the code.\"\"\"\n",
    "        optimized_code = code\n",
    "        \n",
    "        # Apply optimization patterns\n",
    "        for lib_type, patterns in self.optimization_patterns.items():\n",
    "            for old_pattern, new_pattern in patterns.items():\n",
    "                optimized_code = optimized_code.replace(old_pattern, new_pattern)\n",
    "        \n",
    "        # Add GPU-specific optimizations\n",
    "        if 'import cupy' in optimized_code and 'import cupy as np' not in optimized_code:\n",
    "            optimized_code = 'import cupy as cp\\\\n' + optimized_code\n",
    "        \n",
    "        # Add memory pool for better performance\n",
    "        if 'cupy' in optimized_code:\n",
    "            memory_pool_code = \"\"\"\n",
    "# Enable CuPy memory pool for better performance\n",
    "import cupy\n",
    "mempool = cupy.get_default_memory_pool()\n",
    "pinned_mempool = cupy.get_default_pinned_memory_pool()\n",
    "\"\"\"\n",
    "            optimized_code = memory_pool_code + optimized_code\n",
    "        \n",
    "        return optimized_code\n",
    "    \n",
    "    def create_benchmark_code(self, original_code: str, optimized_code: str) -> Tuple[str, str]:\n",
    "        \"\"\"Create side-by-side benchmark versions.\"\"\"\n",
    "        \n",
    "        cpu_benchmark = f\"\"\"\n",
    "# CPU Version Benchmark\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "{original_code}\n",
    "\"\"\"\n",
    "        \n",
    "        gpu_benchmark = f\"\"\"\n",
    "# GPU Version Benchmark  \n",
    "import time\n",
    "import cupy as cp\n",
    "import cudf as pd\n",
    "\n",
    "{optimized_code}\n",
    "\n",
    "# Convert final results back to CPU for comparison if needed\n",
    "# result = cp.asnumpy(result) if hasattr(result, 'get') else result\n",
    "\"\"\"\n",
    "        \n",
    "        return cpu_benchmark, gpu_benchmark\n",
    "\n",
    "# Initialize the code optimizer\n",
    "code_optimizer = CodeOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22780b6",
   "metadata": {},
   "source": [
    "## 13. Benchmark Engine - Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29bda287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkEngine:\n",
    "    \"\"\"\n",
    "    Coordinates CPU vs GPU benchmarking using Sol's compute resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sol_executor: SolCodeExecutor, code_optimizer: CodeOptimizer):\n",
    "        self.sol_executor = sol_executor\n",
    "        self.code_optimizer = code_optimizer\n",
    "        self.benchmark_history = []\n",
    "    \n",
    "    def run_comprehensive_benchmark(self, user_code: str, timeout: int = 300) -> Dict:\n",
    "        \"\"\"\n",
    "        Run comprehensive CPU vs GPU benchmark.\n",
    "        \n",
    "        Args:\n",
    "            user_code: Original user code to benchmark\n",
    "            timeout: Maximum wait time for jobs to complete (seconds)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with benchmark results and visualizations\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"🔍 Analyzing code for optimization opportunities...\")\n",
    "        analysis = self.code_optimizer.analyze_code(user_code)\n",
    "        \n",
    "        print(\"⚡ Generating GPU-optimized version...\")\n",
    "        optimized_code = self.code_optimizer.suggest_optimizations(user_code)\n",
    "        \n",
    "        # Create benchmark versions\n",
    "        cpu_code, gpu_code = self.code_optimizer.create_benchmark_code(user_code, optimized_code)\n",
    "        \n",
    "        print(\"🚀 Submitting jobs to Sol...\")\n",
    "        \n",
    "        # Submit CPU job\n",
    "        cpu_script, cpu_job_id = self.sol_executor.create_slurm_script(\n",
    "            cpu_code, job_type=\"cpu\", time_limit=\"00:15:00\"\n",
    "        )\n",
    "        cpu_slurm_id = self.sol_executor.submit_job(cpu_script, cpu_job_id)\n",
    "        \n",
    "        # Submit GPU job\n",
    "        gpu_script, gpu_job_id = self.sol_executor.create_slurm_script(\n",
    "            gpu_code, job_type=\"gpu\", time_limit=\"00:15:00\"\n",
    "        )\n",
    "        gpu_slurm_id = self.sol_executor.submit_job(gpu_script, gpu_job_id)\n",
    "        \n",
    "        if not cpu_slurm_id or not gpu_slurm_id:\n",
    "            return {\"error\": \"Failed to submit jobs to Sol\"}\n",
    "        \n",
    "        print(f\"✅ Jobs submitted: CPU ({cpu_slurm_id}), GPU ({gpu_slurm_id})\")\n",
    "        print(\"⏳ Waiting for jobs to complete...\")\n",
    "        \n",
    "        # Wait for jobs to complete\n",
    "        start_wait = time.time()\n",
    "        cpu_status = gpu_status = \"PENDING\"\n",
    "        \n",
    "        while time.time() - start_wait < timeout:\n",
    "            cpu_status = self.sol_executor.check_job_status(cpu_slurm_id)\n",
    "            gpu_status = self.sol_executor.check_job_status(gpu_slurm_id)\n",
    "            \n",
    "            print(f\"📊 Status - CPU: {cpu_status}, GPU: {gpu_status}\")\n",
    "            \n",
    "            if cpu_status in [\"COMPLETED\", \"FAILED\"] and gpu_status in [\"COMPLETED\", \"FAILED\"]:\n",
    "                break\n",
    "                \n",
    "            time.sleep(10)  # Check every 10 seconds\n",
    "        \n",
    "        # Collect results\n",
    "        print(\"📈 Collecting benchmark results...\")\n",
    "        cpu_results = self.sol_executor.get_job_results(cpu_job_id, \"cpu\")\n",
    "        gpu_results = self.sol_executor.get_job_results(gpu_job_id, \"gpu\")\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        benchmark_results = self._process_results(\n",
    "            cpu_results, gpu_results, analysis, user_code, optimized_code\n",
    "        )\n",
    "        \n",
    "        # Store in history\n",
    "        self.benchmark_history.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"results\": benchmark_results\n",
    "        })\n",
    "        \n",
    "        # Cleanup\n",
    "        self.sol_executor.cleanup_job_files(cpu_job_id)\n",
    "        self.sol_executor.cleanup_job_files(gpu_job_id)\n",
    "        \n",
    "        return benchmark_results\n",
    "    \n",
    "    def _process_results(self, cpu_results: Dict, gpu_results: Dict, \n",
    "                        analysis: Dict, original_code: str, optimized_code: str) -> Dict:\n",
    "        \"\"\"Process and format benchmark results.\"\"\"\n",
    "        \n",
    "        results = {\n",
    "            \"analysis\": analysis,\n",
    "            \"original_code\": original_code,\n",
    "            \"optimized_code\": optimized_code,\n",
    "            \"cpu_results\": cpu_results,\n",
    "            \"gpu_results\": gpu_results,\n",
    "            \"performance_metrics\": {},\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        if (cpu_results.get(\"status\") == \"success\" and \n",
    "            gpu_results.get(\"status\") == \"success\"):\n",
    "            \n",
    "            cpu_time = cpu_results.get(\"execution_time\", 0)\n",
    "            gpu_time = gpu_results.get(\"execution_time\", 0)\n",
    "            \n",
    "            if cpu_time > 0 and gpu_time > 0:\n",
    "                speedup = cpu_time / gpu_time\n",
    "                efficiency = (speedup / analysis.get(\"estimated_speedup\", 1.0)) * 100\n",
    "                \n",
    "                results[\"performance_metrics\"] = {\n",
    "                    \"cpu_execution_time\": cpu_time,\n",
    "                    \"gpu_execution_time\": gpu_time,\n",
    "                    \"speedup_factor\": speedup,\n",
    "                    \"efficiency_percent\": efficiency,\n",
    "                    \"time_saved\": cpu_time - gpu_time,\n",
    "                    \"percent_improvement\": ((cpu_time - gpu_time) / cpu_time) * 100\n",
    "                }\n",
    "        \n",
    "        # Generate recommendations\n",
    "        results[\"recommendations\"] = self._generate_recommendations(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _generate_recommendations(self, results: Dict) -> List[str]:\n",
    "        \"\"\"Generate educational recommendations based on benchmark results.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        metrics = results.get(\"performance_metrics\", {})\n",
    "        speedup = metrics.get(\"speedup_factor\", 1.0)\n",
    "        \n",
    "        if speedup > 5:\n",
    "            recommendations.append(\"🎉 Excellent GPU acceleration! This workload benefits significantly from parallel processing.\")\n",
    "        elif speedup > 2:\n",
    "            recommendations.append(\"✅ Good GPU speedup achieved. Consider optimizing memory access patterns for even better performance.\")\n",
    "        elif speedup > 1.1:\n",
    "            recommendations.append(\"📈 Modest improvement with GPU. This workload may be memory-bound or have limited parallelism.\")\n",
    "        else:\n",
    "            recommendations.append(\"⚠️ Limited GPU benefit. Consider if this workload has sufficient computational complexity.\")\n",
    "        \n",
    "        # Check for optimization opportunities\n",
    "        analysis = results.get(\"analysis\", {})\n",
    "        if \"numpy\" in analysis.get(\"libraries_detected\", []):\n",
    "            recommendations.append(\"💡 Consider using CuPy's memory pool for better performance with repeated operations.\")\n",
    "        \n",
    "        if \"pandas\" in analysis.get(\"libraries_detected\", []):\n",
    "            recommendations.append(\"📊 cuDF provides GPU-accelerated dataframe operations similar to pandas.\")\n",
    "        \n",
    "        if analysis.get(\"warnings\"):\n",
    "            recommendations.append(\"⚠️ Some operations may require CPU-GPU memory transfers. Profile memory usage.\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def create_visualization(self, benchmark_results: Dict) -> go.Figure:\n",
    "        \"\"\"Create interactive visualization of benchmark results.\"\"\"\n",
    "        \n",
    "        metrics = benchmark_results.get(\"performance_metrics\", {})\n",
    "        \n",
    "        if not metrics:\n",
    "            # Create error visualization\n",
    "            fig = go.Figure()\n",
    "            fig.add_annotation(\n",
    "                text=\"Benchmark data not available\",\n",
    "                xref=\"paper\", yref=\"paper\",\n",
    "                x=0.5, y=0.5, showarrow=False,\n",
    "                font=dict(size=20)\n",
    "            )\n",
    "            return fig\n",
    "        \n",
    "        # Create comparison chart\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Execution time comparison\n",
    "        fig.add_trace(go.Bar(\n",
    "            name='CPU',\n",
    "            x=['Execution Time'],\n",
    "            y=[metrics[\"cpu_execution_time\"]],\n",
    "            marker_color='lightcoral',\n",
    "            text=[f\"{metrics['cpu_execution_time']:.3f}s\"],\n",
    "            textposition='auto'\n",
    "        ))\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            name='GPU',\n",
    "            x=['Execution Time'],\n",
    "            y=[metrics[\"gpu_execution_time\"]],\n",
    "            marker_color='lightblue',\n",
    "            text=[f\"{metrics['gpu_execution_time']:.3f}s\"],\n",
    "            textposition='auto'\n",
    "        ))\n",
    "        \n",
    "        # Add speedup annotation\n",
    "        speedup = metrics.get(\"speedup_factor\", 1.0)\n",
    "        fig.add_annotation(\n",
    "            text=f\"🚀 {speedup:.1f}x Speedup\",\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.7, y=0.9,\n",
    "            showarrow=False,\n",
    "            font=dict(size=16, color=\"green\"),\n",
    "            bgcolor=\"lightyellow\",\n",
    "            bordercolor=\"orange\",\n",
    "            borderwidth=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"CPU vs GPU Performance Comparison\",\n",
    "            yaxis_title=\"Execution Time (seconds)\",\n",
    "            barmode='group',\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Initialize the benchmark engine\n",
    "benchmark_engine = BenchmarkEngine(sol_executor, code_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a15023",
   "metadata": {},
   "source": [
    "## 14. Enhanced GPU Mentor Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06beda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedGPUMentor:\n",
    "    \"\"\"\n",
    "    Enhanced GPU Mentor that combines RAG capabilities with code execution and analysis.\n",
    "    Integrates code input directly with LLM for comprehensive responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_graph, benchmark_engine: BenchmarkEngine, code_optimizer: CodeOptimizer):\n",
    "        self.rag_graph = rag_graph\n",
    "        self.benchmark_engine = benchmark_engine\n",
    "        self.code_optimizer = code_optimizer\n",
    "        self.conversation_history = []\n",
    "        self.code_execution_results = []\n",
    "    \n",
    "    def process_user_input(self, user_input: str, code: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Process user input with optional code, feeding both to LLM for integrated response.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = {\n",
    "            \"text_response\": \"\",\n",
    "            \"code_analysis\": None,\n",
    "            \"code_output\": None,\n",
    "            \"optimized_code\": None,\n",
    "            \"socratic_questions\": [],\n",
    "            \"learning_objectives\": []\n",
    "        }\n",
    "        \n",
    "        # Create enhanced prompt that includes code context if provided\n",
    "        enhanced_prompt = self._create_enhanced_prompt(user_input, code)\n",
    "        \n",
    "        # Get RAG response with code context\n",
    "        rag_result = self.rag_graph.invoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": enhanced_prompt}]\n",
    "        })\n",
    "        response[\"text_response\"] = rag_result[\"messages\"][-1].content\n",
    "        \n",
    "        # If code is provided, analyze and execute it\n",
    "        if code and code.strip():\n",
    "            print(\"🔍 Analyzing provided code...\")\n",
    "            \n",
    "            # Analyze code for optimization opportunities\n",
    "            analysis = self.code_optimizer.analyze_code(code)\n",
    "            response[\"code_analysis\"] = analysis\n",
    "            \n",
    "            # Generate optimized version\n",
    "            optimized_code = self.code_optimizer.suggest_optimizations(code)\n",
    "            response[\"optimized_code\"] = optimized_code\n",
    "            \n",
    "            # Execute code and capture output\n",
    "            print(\"⚡ Executing code...\")\n",
    "            try:\n",
    "                code_output = self._execute_code_safely(code)\n",
    "                response[\"code_output\"] = code_output\n",
    "                \n",
    "                # Store execution results\n",
    "                self.code_execution_results.append({\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"code\": code,\n",
    "                    \"output\": code_output,\n",
    "                    \"analysis\": analysis\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                response[\"code_output\"] = {\"error\": f\"Code execution failed: {str(e)}\"}\n",
    "            \n",
    "            # Generate educational content based on code and context\n",
    "            response[\"socratic_questions\"] = self._generate_socratic_questions(analysis, user_input, code)\n",
    "            response[\"learning_objectives\"] = self._generate_learning_objectives(analysis, code)\n",
    "        \n",
    "        # Store conversation\n",
    "        self.conversation_history.append({\n",
    "            \"user_input\": user_input,\n",
    "            \"code\": code,\n",
    "            \"response\": response,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _create_enhanced_prompt(self, user_input: str, code: str = None) -> str:\n",
    "        \"\"\"Create enhanced prompt that includes code context for the LLM.\"\"\"\n",
    "        \n",
    "        if not code or not code.strip():\n",
    "            return user_input\n",
    "        \n",
    "        enhanced_prompt = f\"\"\"\n",
    "User Question: {user_input}\n",
    "\n",
    "User's Python Code:\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "Please analyze this code in the context of the user's question. Consider:\n",
    "1. How the code relates to the question being asked\n",
    "2. Potential GPU acceleration opportunities in this specific code\n",
    "3. Any issues, optimizations, or improvements you can suggest\n",
    "4. Educational insights about GPU acceleration concepts demonstrated in this code\n",
    "\n",
    "Provide a comprehensive response that addresses both the question and the code together.\n",
    "\"\"\"\n",
    "        return enhanced_prompt\n",
    "    \n",
    "    def _execute_code_safely(self, code: str) -> Dict:\n",
    "        \"\"\"Execute code safely and capture output.\"\"\"\n",
    "        \n",
    "        import io\n",
    "        import sys\n",
    "        import contextlib\n",
    "        \n",
    "        # Capture stdout and stderr\n",
    "        old_stdout = sys.stdout\n",
    "        old_stderr = sys.stderr\n",
    "        stdout_capture = io.StringIO()\n",
    "        stderr_capture = io.StringIO()\n",
    "        \n",
    "        execution_result = {\n",
    "            \"stdout\": \"\",\n",
    "            \"stderr\": \"\",\n",
    "            \"variables\": {},\n",
    "            \"execution_time\": 0,\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            # Redirect output\n",
    "            sys.stdout = stdout_capture\n",
    "            sys.stderr = stderr_capture\n",
    "            \n",
    "            # Create a safe execution environment\n",
    "            safe_globals = {\n",
    "                '__builtins__': __builtins__,\n",
    "                'print': print,\n",
    "                'len': len,\n",
    "                'range': range,\n",
    "                'enumerate': enumerate,\n",
    "                'zip': zip,\n",
    "                'sum': sum,\n",
    "                'max': max,\n",
    "                'min': min,\n",
    "                'abs': abs,\n",
    "                'round': round,\n",
    "                'type': type,\n",
    "                'str': str,\n",
    "                'int': int,\n",
    "                'float': float,\n",
    "                'list': list,\n",
    "                'dict': dict,\n",
    "                'tuple': tuple,\n",
    "                'set': set,\n",
    "            }\n",
    "            \n",
    "            # Add commonly used libraries\n",
    "            try:\n",
    "                import numpy as np\n",
    "                safe_globals['np'] = np\n",
    "                safe_globals['numpy'] = np\n",
    "            except ImportError:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                import pandas as pd\n",
    "                safe_globals['pd'] = pd\n",
    "                safe_globals['pandas'] = pd\n",
    "            except ImportError:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "                safe_globals['plt'] = plt\n",
    "            except ImportError:\n",
    "                pass\n",
    "            \n",
    "            local_vars = {}\n",
    "            \n",
    "            # Execute the code\n",
    "            exec(code, safe_globals, local_vars)\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            execution_result[\"execution_time\"] = end_time - start_time\n",
    "            \n",
    "            # Capture variables (limit to avoid memory issues)\n",
    "            for name, value in local_vars.items():\n",
    "                if not name.startswith('_'):\n",
    "                    try:\n",
    "                        # Only store basic info about complex objects\n",
    "                        if hasattr(value, 'shape'):  # numpy arrays, pandas objects\n",
    "                            execution_result[\"variables\"][name] = f\"{type(value).__name__} with shape {value.shape}\"\n",
    "                        elif hasattr(value, '__len__') and len(value) > 100:\n",
    "                            execution_result[\"variables\"][name] = f\"{type(value).__name__} with {len(value)} elements\"\n",
    "                        elif isinstance(value, (int, float, str, bool, list, dict, tuple)) and len(str(value)) < 1000:\n",
    "                            execution_result[\"variables\"][name] = str(value)\n",
    "                        else:\n",
    "                            execution_result[\"variables\"][name] = f\"{type(value).__name__} object\"\n",
    "                    except:\n",
    "                        execution_result[\"variables\"][name] = f\"{type(value).__name__} object\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            execution_result[\"status\"] = \"error\"\n",
    "            execution_result[\"error\"] = str(e)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_result[\"execution_time\"] = end_time - start_time\n",
    "        \n",
    "        finally:\n",
    "            # Restore stdout/stderr\n",
    "            sys.stdout = old_stdout\n",
    "            sys.stderr = old_stderr\n",
    "            \n",
    "            # Capture output\n",
    "            execution_result[\"stdout\"] = stdout_capture.getvalue()\n",
    "            execution_result[\"stderr\"] = stderr_capture.getvalue()\n",
    "        \n",
    "        return execution_result\n",
    "    \n",
    "    def _generate_socratic_questions(self, analysis: Dict, user_context: str, code: str) -> List[str]:\n",
    "        \"\"\"Generate Socratic questions based on code analysis and user context.\"\"\"\n",
    "        questions = []\n",
    "        \n",
    "        libraries = analysis.get(\"libraries_detected\", [])\n",
    "        estimated_speedup = analysis.get(\"estimated_speedup\", 1.0)\n",
    "        \n",
    "        # Code-specific questions\n",
    "        if \"numpy\" in libraries:\n",
    "            questions.extend([\n",
    "                \"Looking at your NumPy operations, which ones do you think would benefit most from GPU acceleration?\",\n",
    "                \"How might the memory access patterns in your code affect GPU performance?\",\n",
    "                \"What would happen to performance if you increased the array sizes by 10x?\"\n",
    "            ])\n",
    "        \n",
    "        if \"pandas\" in libraries:\n",
    "            questions.extend([\n",
    "                \"Which pandas operations in your code are most computationally expensive?\",\n",
    "                \"How would you modify this code to work with cuDF instead of pandas?\",\n",
    "                \"What considerations should you make when transferring data between CPU and GPU?\"\n",
    "            ])\n",
    "        \n",
    "        # Context-aware questions\n",
    "        if \"for \" in code and \"range(\" in code:\n",
    "            questions.append(\"Could you vectorize any of these loops to improve performance?\")\n",
    "        \n",
    "        if \"def \" in code:\n",
    "            questions.append(\"How could you modify this function to accept both CPU and GPU arrays?\")\n",
    "        \n",
    "        if estimated_speedup > 5:\n",
    "            questions.append(\"Your code has high parallelization potential. What makes it suitable for GPU acceleration?\")\n",
    "        elif estimated_speedup < 2:\n",
    "            questions.append(\"This code may not benefit much from GPU acceleration. Can you identify why?\")\n",
    "        \n",
    "        return questions[:3]  # Limit to avoid overwhelming\n",
    "    \n",
    "    def _generate_learning_objectives(self, analysis: Dict, code: str) -> List[str]:\n",
    "        \"\"\"Generate specific learning objectives based on the code and analysis.\"\"\"\n",
    "        objectives = []\n",
    "        \n",
    "        libraries = analysis.get(\"libraries_detected\", [])\n",
    "        \n",
    "        if \"numpy\" in libraries:\n",
    "            objectives.extend([\n",
    "                \"Understand when to use CuPy vs NumPy for your specific operations\",\n",
    "                \"Learn about GPU memory management for array operations\",\n",
    "                \"Master efficient data transfer between CPU and GPU\"\n",
    "            ])\n",
    "        \n",
    "        if \"pandas\" in libraries:\n",
    "            objectives.extend([\n",
    "                \"Compare cuDF vs pandas for your data processing workflow\",\n",
    "                \"Understand GPU memory requirements for dataframe operations\",\n",
    "                \"Learn efficient groupby and aggregation patterns on GPU\"\n",
    "            ])\n",
    "        \n",
    "        # Code-specific objectives\n",
    "        if \"for \" in code:\n",
    "            objectives.append(\"Explore vectorization techniques to eliminate loops\")\n",
    "        \n",
    "        if \"def \" in code:\n",
    "            objectives.append(\"Design functions that work efficiently with both CPU and GPU data\")\n",
    "        \n",
    "        return objectives\n",
    "    \n",
    "    def generate_tutorial_content(self, topic: str) -> str:\n",
    "        \"\"\"Generate comprehensive tutorial content on specific GPU acceleration topics.\"\"\"\n",
    "        \n",
    "        tutorial_prompt = f\"\"\"\n",
    "        Create a comprehensive tutorial on {topic} for GPU acceleration. Include:\n",
    "        1. Conceptual explanation\n",
    "        2. Code examples comparing CPU vs GPU approaches\n",
    "        3. Performance considerations\n",
    "        4. Best practices\n",
    "        5. Common pitfalls to avoid\n",
    "        \n",
    "        Focus on practical, hands-on learning with RAPIDS and CuPy libraries.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.rag_graph.invoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": tutorial_prompt}]\n",
    "        })\n",
    "        \n",
    "        return result[\"messages\"][-1].content\n",
    "    \n",
    "    def get_execution_summary(self) -> Dict:\n",
    "        \"\"\"Get summary of all code execution results.\"\"\"\n",
    "        if not self.code_execution_results:\n",
    "            return {\"message\": \"No code executed yet\"}\n",
    "        \n",
    "        summary = {\n",
    "            \"total_executions\": len(self.code_execution_results),\n",
    "            \"successful_executions\": 0,\n",
    "            \"failed_executions\": 0,\n",
    "            \"average_execution_time\": 0,\n",
    "            \"common_libraries\": [],\n",
    "            \"recent_outputs\": []\n",
    "        }\n",
    "        \n",
    "        execution_times = []\n",
    "        libraries_count = {}\n",
    "        \n",
    "        for result in self.code_execution_results[-10:]:  # Last 10 executions\n",
    "            if result.get(\"output\", {}).get(\"status\") == \"success\":\n",
    "                summary[\"successful_executions\"] += 1\n",
    "                exec_time = result.get(\"output\", {}).get(\"execution_time\", 0)\n",
    "                execution_times.append(exec_time)\n",
    "            else:\n",
    "                summary[\"failed_executions\"] += 1\n",
    "            \n",
    "            # Count libraries\n",
    "            for lib in result.get(\"analysis\", {}).get(\"libraries_detected\", []):\n",
    "                libraries_count[lib] = libraries_count.get(lib, 0) + 1\n",
    "            \n",
    "            # Add recent output summary\n",
    "            output = result.get(\"output\", {})\n",
    "            summary[\"recent_outputs\"].append({\n",
    "                \"timestamp\": result.get(\"timestamp\"),\n",
    "                \"status\": output.get(\"status\", \"unknown\"),\n",
    "                \"execution_time\": output.get(\"execution_time\", 0),\n",
    "                \"output_length\": len(output.get(\"stdout\", \"\"))\n",
    "            })\n",
    "        \n",
    "        if execution_times:\n",
    "            summary[\"average_execution_time\"] = sum(execution_times) / len(execution_times)\n",
    "        \n",
    "        summary[\"common_libraries\"] = sorted(libraries_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Initialize the enhanced GPU mentor\n",
    "gpu_mentor = EnhancedGPUMentor(graph, benchmark_engine, code_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e3557",
   "metadata": {},
   "source": [
    "## 15. Enhanced Gradio Interface - GPU Mentor Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3052fad3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Code.__init__() got an unexpected keyword argument 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 267\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# Code input area (collapsible)\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m gr.Accordion(\u001b[33m\"\u001b[39m\u001b[33m📝 Python Code (Optional)\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mopen\u001b[39m=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     code_input = \u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpython\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_label\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mplaceholder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m# Enter your Python code here (optional)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# The AI will analyze and execute it along with your question\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# Sample code selector\u001b[39;00m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m gr.Row():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/packages/envs/genai25.06/lib/python3.12/site-packages/gradio/component_meta.py:185\u001b[39m, in \u001b[36mupdateable.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Code.__init__() got an unexpected keyword argument 'placeholder'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "\n",
    "def chat_with_mentor(message, code, chat_history):\n",
    "    \"\"\"Handle chat interactions with the GPU Mentor - now integrates code with LLM.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Process user input through the enhanced mentor (code + message together)\n",
    "        response = gpu_mentor.process_user_input(message, code)\n",
    "        \n",
    "        # Format response for chat\n",
    "        formatted_response = response[\"text_response\"]\n",
    "        \n",
    "        # Add code analysis if available\n",
    "        if response[\"code_analysis\"]:\n",
    "            analysis = response[\"code_analysis\"]\n",
    "            formatted_response += f\"\\n\\n**📊 Code Analysis:**\\n\"\n",
    "            formatted_response += f\"• Libraries detected: {', '.join(analysis['libraries_detected'])}\\n\"\n",
    "            formatted_response += f\"• Estimated speedup potential: {analysis['estimated_speedup']:.1f}x\\n\"\n",
    "            formatted_response += f\"• GPU compatible: {'✅' if analysis['gpu_compatible'] else '❌'}\\n\"\n",
    "            \n",
    "            if analysis['warnings']:\n",
    "                formatted_response += f\"• ⚠️ Warnings: {'; '.join(analysis['warnings'])}\\n\"\n",
    "        \n",
    "        # Add code execution output\n",
    "        if response[\"code_output\"]:\n",
    "            output = response[\"code_output\"]\n",
    "            formatted_response += f\"\\n\\n**⚡ Code Execution Results:**\\n\"\n",
    "            \n",
    "            if output.get(\"status\") == \"success\":\n",
    "                formatted_response += f\"• ✅ Execution successful ({output.get('execution_time', 0):.3f}s)\\n\"\n",
    "                \n",
    "                if output.get(\"stdout\"):\n",
    "                    formatted_response += f\"• 📄 Output:\\n```\\n{output['stdout']}\\n```\\n\"\n",
    "                \n",
    "                if output.get(\"variables\"):\n",
    "                    formatted_response += f\"• 📊 Variables created: {', '.join(output['variables'].keys())}\\n\"\n",
    "                    # Show details for important variables\n",
    "                    for var_name, var_info in list(output['variables'].items())[:3]:\n",
    "                        formatted_response += f\"  - `{var_name}`: {var_info}\\n\"\n",
    "            else:\n",
    "                formatted_response += f\"• ❌ Execution failed: {output.get('error', 'Unknown error')}\\n\"\n",
    "                if output.get(\"stderr\"):\n",
    "                    formatted_response += f\"• 🚨 Error details:\\n```\\n{output['stderr']}\\n```\\n\"\n",
    "        \n",
    "        # Add Socratic questions\n",
    "        if response[\"socratic_questions\"]:\n",
    "            formatted_response += f\"\\n\\n**🤔 Think About This:**\\n\"\n",
    "            for i, question in enumerate(response[\"socratic_questions\"], 1):\n",
    "                formatted_response += f\"{i}. {question}\\n\"\n",
    "        \n",
    "        # Update chat history\n",
    "        if chat_history is None:\n",
    "            chat_history = []\n",
    "        \n",
    "        # Format user message with code if provided\n",
    "        user_message = message\n",
    "        if code and code.strip():\n",
    "            user_message += f\"\\n\\n```python\\n{code}\\n```\"\n",
    "        \n",
    "        chat_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": formatted_response})\n",
    "        \n",
    "        return \"\", \"\", chat_history, response.get(\"code_output\"), response.get(\"optimized_code\", \"\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"❌ Error: {str(e)}\"\n",
    "        if chat_history is None:\n",
    "            chat_history = []\n",
    "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": error_msg})\n",
    "        return \"\", \"\", chat_history, None, \"\"\n",
    "\n",
    "def analyze_code_only(code):\n",
    "    \"\"\"Analyze code for optimization opportunities.\"\"\"\n",
    "    \n",
    "    if not code.strip():\n",
    "        return \"Please provide code to analyze.\", \"\"\n",
    "    \n",
    "    try:\n",
    "        analysis = code_optimizer.analyze_code(code)\n",
    "        optimized_code = code_optimizer.suggest_optimizations(code)\n",
    "        \n",
    "        analysis_text = f\"\"\"\n",
    "**🔍 Code Analysis Results:**\n",
    "• Libraries detected: {', '.join(analysis['libraries_detected'])}\n",
    "• Estimated speedup potential: {analysis['estimated_speedup']:.1f}x\n",
    "• GPU compatible: {'✅ Yes' if analysis['gpu_compatible'] else '❌ No'}\n",
    "\n",
    "**⚡ Optimization Opportunities:**\n",
    "• Matrix operations: {'✅ Detected' if any(op in code for op in ['np.dot', 'np.matmul', '@']) else '❌ None'}\n",
    "• Array operations: {'✅ Detected' if 'numpy' in analysis['libraries_detected'] else '❌ None'}\n",
    "• DataFrame operations: {'✅ Detected' if 'pandas' in analysis['libraries_detected'] else '❌ None'}\n",
    "• Loop vectorization: {'✅ Possible' if 'for ' in code and 'range(' in code else '❌ None'}\n",
    "\n",
    "**⚠️ Considerations:**\n",
    "{chr(10).join('• ' + warning for warning in analysis['warnings']) if analysis['warnings'] else '• None detected'}\n",
    "\n",
    "**💡 Recommendations:**\n",
    "• Consider using CuPy for NumPy operations on large arrays\n",
    "• Try cuDF for pandas operations on large datasets  \n",
    "• Use memory pools for repeated GPU operations\n",
    "• Profile memory usage for optimal batch sizes\n",
    "\"\"\"\n",
    "        \n",
    "        return analysis_text, optimized_code\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing code: {str(e)}\", \"\"\n",
    "\n",
    "def get_tutorial(topic):\n",
    "    \"\"\"Generate tutorial content for specific topics.\"\"\"\n",
    "    \n",
    "    if not topic.strip():\n",
    "        return \"Please specify a topic for the tutorial.\"\n",
    "    \n",
    "    try:\n",
    "        tutorial_content = gpu_mentor.generate_tutorial_content(topic)\n",
    "        return tutorial_content\n",
    "    except Exception as e:\n",
    "        return f\"Error generating tutorial: {str(e)}\"\n",
    "\n",
    "def clear_chat():\n",
    "    \"\"\"Clear chat history.\"\"\"\n",
    "    return None, None, None, None, \"\"\n",
    "\n",
    "# Sample code examples for quick testing\n",
    "sample_codes = {\n",
    "    \"Simple Array Operations\": '''import numpy as np\n",
    "\n",
    "# Create arrays\n",
    "n = 1000\n",
    "x = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "\n",
    "# Basic operations\n",
    "result = np.sqrt(x**2 + y**2)\n",
    "mean_result = np.mean(result)\n",
    "\n",
    "print(f\"Array size: {n}\")\n",
    "print(f\"Mean result: {mean_result:.4f}\")\n",
    "print(f\"Max result: {np.max(result):.4f}\")''',\n",
    "\n",
    "    \"Matrix Multiplication\": '''import numpy as np\n",
    "\n",
    "# Create matrices\n",
    "n = 500\n",
    "A = np.random.rand(n, n)\n",
    "B = np.random.rand(n, n)\n",
    "\n",
    "# Matrix multiplication\n",
    "C = np.dot(A, B)\n",
    "\n",
    "print(f\"Matrix size: {n}x{n}\")\n",
    "print(f\"Result shape: {C.shape}\")\n",
    "print(f\"Result sum: {np.sum(C):.2f}\")''',\n",
    "    \n",
    "    \"DataFrame Operations\": '''import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create dataset\n",
    "n = 10000\n",
    "df = pd.DataFrame({\n",
    "    'x': np.random.randn(n),\n",
    "    'y': np.random.randn(n),\n",
    "    'group': np.random.choice(['A', 'B', 'C'], n)\n",
    "})\n",
    "\n",
    "# Compute statistics\n",
    "result = df.groupby('group').agg({\n",
    "    'x': ['mean', 'std'],\n",
    "    'y': ['sum', 'count']\n",
    "})\n",
    "\n",
    "print(f\"Dataset size: {len(df)} rows\")\n",
    "print(\"Grouped results:\")\n",
    "print(result)''',\n",
    "    \n",
    "    \"Mathematical Functions\": '''import numpy as np\n",
    "\n",
    "# Generate data\n",
    "n = 5000\n",
    "x = np.linspace(0, 4*np.pi, n)\n",
    "y = np.sin(x) * np.exp(-x/10)\n",
    "\n",
    "# Compute statistics\n",
    "mean_y = np.mean(y)\n",
    "std_y = np.std(y)\n",
    "max_y = np.max(y)\n",
    "\n",
    "print(f\"Data points: {n}\")\n",
    "print(f\"Mean: {mean_y:.4f}\")\n",
    "print(f\"Std: {std_y:.4f}\")\n",
    "print(f\"Max: {max_y:.4f}\")''',\n",
    "\n",
    "    \"Data Processing Loop\": '''import numpy as np\n",
    "\n",
    "# Create data\n",
    "data = np.random.rand(1000, 10)\n",
    "results = []\n",
    "\n",
    "# Process data (can be vectorized)\n",
    "for i in range(len(data)):\n",
    "    row_sum = np.sum(data[i])\n",
    "    row_mean = np.mean(data[i])\n",
    "    results.append(row_sum * row_mean)\n",
    "\n",
    "final_result = np.array(results)\n",
    "print(f\"Processed {len(data)} rows\")\n",
    "print(f\"Final result shape: {final_result.shape}\")\n",
    "print(f\"Average result: {np.mean(final_result):.4f}\")'''\n",
    "}\n",
    "\n",
    "# Create the enhanced Gradio interface\n",
    "with gr.Blocks(title=\"GPU Mentor - Enhanced AI Tutor\", theme=gr.themes.Soft()) as demo:\n",
    "    \n",
    "    gr.Markdown(\"# 🚀 Enhanced GPU Mentor: AI Tutor with Integrated Code Execution\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        \n",
    "        # Features Tab (moved from main interface)\n",
    "        with gr.Tab(\"ℹ️ Features\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ## 🚀 Enhanced GPU Mentor Features\n",
    "            \n",
    "            **🔗 Integrated LLM + Code**: Ask questions about your code - the AI sees both your question and code together\n",
    "            \n",
    "            **⚡ Live Code Execution**: Run your Python code instantly and see the output in the chat\n",
    "            \n",
    "            **🔍 Smart Analysis**: Get optimization suggestions and GPU acceleration opportunities\n",
    "            \n",
    "            **📚 Educational Guidance**: Socratic questions and learning objectives based on your actual code\n",
    "            \n",
    "            **🎯 Multi-Modal Support**: Handles text questions, code analysis, and execution all in one interface\n",
    "            \n",
    "            **🚀 GPU Optimization**: Automatic detection of optimization opportunities and GPU-compatible code suggestions\n",
    "            \n",
    "            **📊 Performance Insights**: Real-time analysis of code performance and potential speedup estimates\n",
    "            \"\"\")\n",
    "        \n",
    "        # Main Chat Playground Tab (Redesigned)\n",
    "        with gr.Tab(\"💬 Chat Playground\"):\n",
    "            with gr.Column():\n",
    "                # Main conversation area\n",
    "                chatbot = gr.Chatbot(\n",
    "                    label=\"GPU Mentor Conversation\",\n",
    "                    height=500,\n",
    "                    type=\"messages\",\n",
    "                    show_copy_button=True\n",
    "                )\n",
    "                \n",
    "                # Integrated input area at bottom of conversation\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=3):\n",
    "                        message_input = gr.Textbox(\n",
    "                            label=\"\",\n",
    "                            placeholder=\"Ask about GPU acceleration, optimization, or explain your code...\",\n",
    "                            lines=2,\n",
    "                            show_label=False\n",
    "                        )\n",
    "                    with gr.Column(scale=1):\n",
    "                        submit_btn = gr.Button(\"💬 Send\", variant=\"primary\", size=\"lg\")\n",
    "                        clear_btn = gr.Button(\"🧹 Clear\", size=\"sm\")\n",
    "                \n",
    "                # Code input area (collapsible)\n",
    "                with gr.Accordion(\"📝 Python Code (Optional)\", open=False):\n",
    "                    code_input = gr.Code(\n",
    "                        label=\"\",\n",
    "                        language=\"python\",\n",
    "                        lines=8,\n",
    "                        show_label=False,\n",
    "                        # placeholder=\"# Enter your Python code here (optional)\\n# The AI will analyze and execute it along with your question\"\n",
    "                    )\n",
    "                    \n",
    "                    # Sample code selector\n",
    "                    with gr.Row():\n",
    "                        sample_dropdown = gr.Dropdown(\n",
    "                            choices=list(sample_codes.keys()),\n",
    "                            label=\"Load Sample Code\",\n",
    "                            value=None,\n",
    "                            scale=2\n",
    "                        )\n",
    "                        load_sample_btn = gr.Button(\"📂 Load\", scale=1)\n",
    "        \n",
    "        # Code Analysis Tab (Updated)\n",
    "        with gr.Tab(\"🔍 Code Analysis & Optimization\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    analyze_code = gr.Code(\n",
    "                        label=\"Code to Analyze\",\n",
    "                        language=\"python\",\n",
    "                        lines=15\n",
    "                    )\n",
    "                    \n",
    "                    analyze_btn = gr.Button(\"🔍 Analyze Code\", variant=\"primary\")\n",
    "                    \n",
    "                    analysis_results = gr.Textbox(\n",
    "                        label=\"Analysis Results\",\n",
    "                        lines=15\n",
    "                    )\n",
    "                \n",
    "                with gr.Column():\n",
    "                    optimized_code = gr.Code(\n",
    "                        label=\"GPU-Optimized Version\",\n",
    "                        language=\"python\",\n",
    "                        lines=20\n",
    "                    )\n",
    "        \n",
    "        # Tutorial Generator Tab  \n",
    "        with gr.Tab(\"📚 Personalized Tutorials\"):\n",
    "            with gr.Column():\n",
    "                tutorial_topic = gr.Textbox(\n",
    "                    label=\"Tutorial Topic\",\n",
    "                    placeholder=\"e.g., 'CuPy memory management', 'cuDF vs pandas performance', 'vectorizing loops'...\",\n",
    "                    lines=1\n",
    "                )\n",
    "                \n",
    "                generate_tutorial_btn = gr.Button(\"📝 Generate Tutorial\", variant=\"primary\")\n",
    "                \n",
    "                tutorial_content = gr.Markdown(\n",
    "                    label=\"Tutorial Content\",\n",
    "                    value=\"Enter a topic above to generate a personalized tutorial.\"\n",
    "                )\n",
    "        \n",
    "        # Execution Summary Tab (Replaces Performance)\n",
    "        with gr.Tab(\"📈 Code Execution Summary\"):\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### Your Code Execution History\")\n",
    "                \n",
    "                summary_btn = gr.Button(\"📊 View Execution Summary\")\n",
    "                execution_summary = gr.JSON(label=\"Execution Summary\")\n",
    "    \n",
    "    # Event handlers\n",
    "    def load_sample_code(sample_name):\n",
    "        if sample_name and sample_name in sample_codes:\n",
    "            return sample_codes[sample_name]\n",
    "        return \"\"\n",
    "    \n",
    "    # Wire up the interface\n",
    "    sample_dropdown.change(load_sample_code, inputs=[sample_dropdown], outputs=[code_input])\n",
    "    load_sample_btn.click(load_sample_code, inputs=[sample_dropdown], outputs=[code_input])\n",
    "    \n",
    "    submit_btn.click(\n",
    "        chat_with_mentor,\n",
    "        inputs=[message_input, code_input, chatbot],\n",
    "        outputs=[message_input, code_input, chatbot, gr.State(), gr.State()]\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(clear_chat, outputs=[chatbot, gr.State(), gr.State()])\n",
    "    \n",
    "    analyze_btn.click(\n",
    "        analyze_code_only,\n",
    "        inputs=[analyze_code],\n",
    "        outputs=[analysis_results, optimized_code]\n",
    "    )\n",
    "    \n",
    "    generate_tutorial_btn.click(\n",
    "        get_tutorial,\n",
    "        inputs=[tutorial_topic],\n",
    "        outputs=[tutorial_content]\n",
    "    )\n",
    "    \n",
    "    summary_btn.click(\n",
    "        lambda: gpu_mentor.get_execution_summary(),\n",
    "        outputs=[execution_summary]\n",
    "    )\n",
    "\n",
    "# Launch the enhanced interface\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e095a615",
   "metadata": {},
   "source": [
    "## 16. Example Usage & Testing\n",
    "\n",
    "Let's test the GPU Mentor system with some example interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dcd7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced GPU Mentor with integrated LLM + code execution\n",
    "sample_numpy_code = \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# Create large arrays\n",
    "n = 1000\n",
    "x = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "\n",
    "# Compute distance\n",
    "result = np.sqrt(x**2 + y**2)\n",
    "mean_distance = np.mean(result)\n",
    "\n",
    "print(f\"Array size: {n}\")\n",
    "print(f\"Mean distance: {mean_distance:.4f}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Testing Enhanced GPU Mentor (LLM + Code Integration) ===\")\n",
    "\n",
    "# Test the integrated approach - LLM sees both question and code together\n",
    "user_question = \"How can I optimize this distance calculation for GPU acceleration?\"\n",
    "\n",
    "try:\n",
    "    # This now sends both the question AND code to the LLM together\n",
    "    response = gpu_mentor.process_user_input(user_question, sample_numpy_code)\n",
    "    \n",
    "    print(\"🤖 AI Response (with code context):\")\n",
    "    print(response[\"text_response\"][:300] + \"...\" if len(response[\"text_response\"]) > 300 else response[\"text_response\"])\n",
    "    \n",
    "    print(\"\\n⚡ Code Execution Results:\")\n",
    "    if response[\"code_output\"]:\n",
    "        output = response[\"code_output\"]\n",
    "        print(f\"Status: {output.get('status', 'unknown')}\")\n",
    "        print(f\"Execution time: {output.get('execution_time', 0):.4f}s\")\n",
    "        print(f\"Output: {output.get('stdout', 'No output')}\")\n",
    "        print(f\"Variables: {list(output.get('variables', {}).keys())}\")\n",
    "    \n",
    "    print(\"\\n🔍 Code Analysis:\")\n",
    "    if response[\"code_analysis\"]:\n",
    "        analysis = response[\"code_analysis\"]\n",
    "        print(f\"Libraries: {analysis.get('libraries_detected', [])}\")\n",
    "        print(f\"Speedup potential: {analysis.get('estimated_speedup', 1.0):.1f}x\")\n",
    "    \n",
    "    print(\"\\n🚀 Optimized Code:\")\n",
    "    if response[\"optimized_code\"]:\n",
    "        print(response[\"optimized_code\"][:200] + \"...\" if len(response[\"optimized_code\"]) > 200 else response[\"optimized_code\"])\n",
    "    \n",
    "    print(\"\\n🤔 Socratic Questions:\")\n",
    "    for i, question in enumerate(response.get(\"socratic_questions\", []), 1):\n",
    "        print(f\"{i}. {question}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error testing enhanced mentor: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing Safe Code Execution...\")\n",
    "\n",
    "# Test safe code execution\n",
    "test_code = \"\"\"\n",
    "import numpy as np\n",
    "data = np.array([1, 2, 3, 4, 5])\n",
    "result = np.sum(data)\n",
    "print(f\"Sum: {result}\")\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    output = gpu_mentor._execute_code_safely(test_code)\n",
    "    print(f\"Execution status: {output['status']}\")\n",
    "    print(f\"Execution time: {output['execution_time']:.4f}s\")\n",
    "    print(f\"Output: {output['stdout']}\")\n",
    "    print(f\"Variables: {output['variables']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in safe execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39546133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced GPU Mentor (without actual Sol execution for demo)\n",
    "print(\"\\n=== Testing Enhanced GPU Mentor ===\")\n",
    "\n",
    "# Simulate a user interaction\n",
    "user_question = \"How can I accelerate matrix multiplication with CuPy?\"\n",
    "sample_code = \"\"\"\n",
    "import numpy as np\n",
    "A = np.random.rand(500, 500)\n",
    "B = np.random.rand(500, 500)\n",
    "C = np.dot(A, B)\n",
    "\"\"\"\n",
    "\n",
    "# Test just the RAG response and code analysis (skip actual benchmarking)\n",
    "try:\n",
    "    # Get RAG response\n",
    "    rag_result = gpu_mentor.rag_graph.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": user_question}]\n",
    "    })\n",
    "    print(\"RAG Response:\", rag_result[\"messages\"][-1].content[:200] + \"...\")\n",
    "    \n",
    "    # Analyze code\n",
    "    analysis = gpu_mentor.code_optimizer.analyze_code(sample_code)\n",
    "    print(\"\\nCode Analysis:\", analysis)\n",
    "    \n",
    "    # Generate Socratic questions\n",
    "    questions = gpu_mentor._generate_socratic_questions(analysis, user_question)\n",
    "    print(\"\\nSocratic Questions:\")\n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f\"{i}. {q}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error testing GPU Mentor: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.06",
   "language": "python",
   "name": "genai25.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
