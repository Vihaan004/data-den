{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f625337",
   "metadata": {},
   "source": [
    "# Agentic RAG with Local Ollama Model\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) agent using LangGraph, LangChain, and a local  model run via Ollama.\n",
    "\n",
    "Adapted from: https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/\n",
    "\n",
    "## Materials\n",
    "This notebook and all materials referenced here can be found on Sol `/data/sse/ai-accelerated-spark`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc8378-b454-48d8-a95c-563521334562",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cfaace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.tools import Tool\n",
    "from langgraph.graph import Graph\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59114acf",
   "metadata": {},
   "source": [
    "## 2. Preprocess documents\n",
    "### 2.1. Fetch documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf7599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://medium.com/cupy-team/announcing-cupy-v13-66979ee7fab0\",\n",
    "    \"https://www.unum.cloud/blog/2022-01-26-cupy\",\n",
    "    \"https://medium.com/rapids-ai/easy-cpu-gpu-arrays-and-dataframes-run-your-dask-code-where-youd-like-e349d92351d\"\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6bc96c-c2c6-4ca8-af17-1a0dc06d46cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0][0].page_content.strip()[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dcb754-10f3-4a16-b522-c1ec4cbd812b",
   "metadata": {},
   "source": [
    "### 2.2. Split the fetched documents into smaller chunks for indexing into the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97649da8-f2bc-49fd-be94-009b59a1a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cad859-5123-4811-b53a-a58ee9be9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_splits[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbf0613",
   "metadata": {},
   "source": [
    "## 3.Create a retriever tool\n",
    "### 3.1. Use an in-memory vector store and all-MiniLM-L6-V2 embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb39231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents = doc_splits, embedding = embedding_model\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb57e5-dcb5-4387-8327-5756c2df4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use ChromaDB for persistent vectorstore\n",
    "# https://python.langchain.com/docs/integrations/vectorstores/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f517c44-2e14-4614-9290-8d75e7faef4f",
   "metadata": {},
   "source": [
    "### 3.2. Create a retriever tool using LangChain's prebuild `create_retriever_tool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5278231-c5eb-40ca-94f5-8a74d99a1b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_python_gpu_acceleration\",\n",
    "    \"Search and return information about accelerating Python code using the GPU with RAPIDS and CuPy.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fcbee6-e9d6-466f-9c7a-303ca012b469",
   "metadata": {},
   "source": [
    "### 3.3. Test the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4b915-9cfc-4e71-9ea2-6060cde12d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool.invoke({\"query\": \"How can I create a CuPy-backed Dask array for random data?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29127ddb-5909-4809-8525-3204ef2d08cf",
   "metadata": {},
   "source": [
    "## 4. Generate query\n",
    "### 4.1. Load local LLM\n",
    "\n",
    "Start ollama using the terminal:\n",
    "```bash\n",
    "module load ollama/0.9.0\n",
    "export OLLAMA_MODELS=/data/datasets/community/ollama\n",
    "ollama-start\n",
    "```\n",
    "\n",
    "Check the available list of models using `ollama list`. Let me know via Slack if you would like to use and test other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4504022-deee-4190-b681-6dfa9ba5e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import socket\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "host_node = socket.gethostname()\n",
    "llm_model = init_chat_model(\"ollama:qwen3:14b\", temperature=0, base_url=f\"http://jgarc111@{host_node}:11434/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972da38e-7439-4d43-8fc6-6aafcdf00069",
   "metadata": {},
   "source": [
    "### 4.2. Build a `generate_query_or_respond` node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ecc05d-868d-4228-bc67-bd8ed7b65a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "import re\n",
    "\n",
    "def generate_query_or_respond(state: MessagesState):\n",
    "    \"\"\"Call the model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n",
    "    \"\"\"\n",
    "    response = (\n",
    "        llm_model\n",
    "        .bind_tools([retriever_tool]).invoke(state[\"messages\"])\n",
    "    )\n",
    "    # remove thinking text\n",
    "    content = re.sub(r\"<think>.*</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
    "    response.content = content\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0091934-a78a-4a46-bdd6-24d62eae69ed",
   "metadata": {},
   "source": [
    "### 4.3. Try a random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc5702-ff03-4934-b664-3d6a8edca463",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\"messages\": [{\"role\": \"user\", \"content\": \"Hello! What is the color of the sky?\"}]}\n",
    "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50e1018-8b5c-405e-ab80-2bfc79e9a7ab",
   "metadata": {},
   "source": [
    "### 4.4. Try semantic search question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934dd1bc-fa2c-414c-80f9-cb94b93ffde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddaf2a9-6dac-4e24-a77d-2ee588ac6eed",
   "metadata": {},
   "source": [
    "## 5. Grade documents\n",
    "### 5.1. Add conditional edge `grade_documents` to determine the relevance of retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6294ac9-5927-4f26-ac89-046829bcdfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "GRADE_PROMPT = (\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n",
    "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
    "    \"Here is the user question: {question} \\n\"\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
    ")\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n",
    "    )\n",
    "\n",
    "\n",
    "def grade_documents(\n",
    "    state: MessagesState,\n",
    ") -> Literal[\"generate_answer\", \"rewrite_question\"]:\n",
    "    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    \n",
    "\n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = (\n",
    "        llm_model\n",
    "        .with_structured_output(GradeDocuments).invoke(\n",
    "            [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    )\n",
    "    score = response.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        return \"rewrite_question\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a918ef9-e933-4f0c-83ce-75d02585a241",
   "metadata": {},
   "source": [
    "### 5.2. Try with irrelevant documents in the tool response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be481b-9484-40db-98c5-f227129cafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
    "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4e35d-63e6-4658-9ade-c6e835c6e39e",
   "metadata": {},
   "source": [
    "### 5.3. Try with relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7119fa-55d5-4179-8bcc-661a68e100b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
    "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": 'Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with dask.config.set({‚Äúarray.backend‚Äù: ‚Äúcupy‚Äù}):‚Ä¶    darr = da.random.randint(0, 3, size=(10, 20), chunks=(2, 5)) #\\n\\n= rs.randint(0, 3, size=(10, 20), chunks=(2, 5))>>> darrdask.array<randint, shape=(10, 20), dtype=int64, chunksize=(2, 5), \\\\chunktype=cupy.ndarray>Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with\\n\\nfor random array creation.',\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6520541-4db0-4db2-839d-66d74494644b",
   "metadata": {},
   "source": [
    "## 6.\n",
    "### 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199cb600-4cad-4e9a-80fc-93746877b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_PROMPT = (\n",
    "    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n",
    "    \"Here is the initial question:\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"Formulate an improved question:\"\n",
    ")\n",
    "\n",
    "\n",
    "def rewrite_question(state: MessagesState):\n",
    "    \"\"\"Rewrite the original user question.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = llm_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    # remove thinking text\n",
    "    content = re.sub(r\"<think>.*</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
    "    response.content = content\n",
    "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025fedf-4e70-443f-acae-c384b7163d8f",
   "metadata": {},
   "source": [
    "### 6.2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f4730-0d74-4172-aaf8-9dc8c55a60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
    "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = rewrite_question(input)\n",
    "print(response[\"messages\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0771d-200e-4a1c-9da3-e8a4e82fb480",
   "metadata": {},
   "source": [
    "## 7. Generate an answer\n",
    "### 7.1. Build `generate_answer` node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3472875-cfd6-4b3f-8b39-7a1139bd7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Question: {question} \\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_answer(state: MessagesState):\n",
    "    \"\"\"Generate an answer.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = llm_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    # remove thinking text\n",
    "    content = re.sub(r\"<think>.*</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
    "    response.content = content\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd1250-820e-40f8-ad58-c85bca8e2c2e",
   "metadata": {},
   "source": [
    "## 7.2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d74558-2403-4c2c-a50c-944cd63dc501",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
    "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": 'Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with dask.config.set({‚Äúarray.backend‚Äù: ‚Äúcupy‚Äù}):‚Ä¶    darr = da.random.randint(0, 3, size=(10, 20), chunks=(2, 5)) #\\n\\n= rs.randint(0, 3, size=(10, 20), chunks=(2, 5))>>> darrdask.array<randint, shape=(10, 20), dtype=int64, chunksize=(2, 5), \\\\chunktype=cupy.ndarray>Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with\\n\\nfor random array creation.',\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = generate_answer(input)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb5fc5f-b09c-42b1-88d9-f2b6784b2eda",
   "metadata": {},
   "source": [
    "## 8. Assemble the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf2038-5fa5-4e22-a208-4b1167e1cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(generate_query_or_respond)\n",
    "workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
    "workflow.add_node(rewrite_question)\n",
    "workflow.add_node(generate_answer)\n",
    "\n",
    "workflow.add_edge(START, \"generate_query_or_respond\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_query_or_respond\",\n",
    "    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44857e0-4ce3-4a02-ba3b-ade019288936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc9c8a-4a28-49dc-ae10-ca7b8eb42e43",
   "metadata": {},
   "source": [
    "## 9. Run the agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d926a1-4f35-42ed-bd08-1027c975644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    for node, update in chunk.items():\n",
    "        print(\"Update from node\", node)\n",
    "        update[\"messages\"][-1].pretty_print()\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131337e-17ea-49e6-a9aa-b0c4798ddfe1",
   "metadata": {},
   "source": [
    "## 10. Graphic User Interface using Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d135bb8-88ae-4cc1-9a5a-5ffde1232826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def ask_graph(user_input, chat_history):\n",
    "    result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    response = result[\"messages\"][-1].content\n",
    "\n",
    "    if not chat_history:\n",
    "        response = [{\"role\": \"user\", \"content\": user_input}, {\"role\": \"assistant\", \"content\": response}]\n",
    "    else:\n",
    "        response = chat_history + [{\"role\": \"user\", \"content\": user_input}, {\"role\": \"assistant\", \"content\": response}]\n",
    "\n",
    "    return \"\", response\n",
    "\n",
    "def clear_conversation():\n",
    "    return \"\", \"\"\n",
    "\n",
    "with gr.Blocks(fill_height=True, fill_width=True) as demo:\n",
    "    gr.Markdown(\"### Agentic RAG\")\n",
    "\n",
    "    with gr.Column():\n",
    "\n",
    "        with gr.Row():\n",
    "            chatbot = gr.Chatbot(height=350, type=\"messages\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=4):\n",
    "                query_input = gr.Textbox(\n",
    "                    label=\"Enter text here\", placeholder=\"Ask something...\", lines=1\n",
    "                    )\n",
    "            with gr.Column(scale=1):\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"‚¨Ü\")\n",
    "                # üßπ Clear button\n",
    "                with gr.Row():\n",
    "                    clear_btn = gr.Button(\"üßπ Clear Conversation\")\n",
    "\n",
    "        submit_btn.click(\n",
    "            fn=ask_graph,\n",
    "            inputs=[query_input, chatbot],\n",
    "            outputs=[query_input, chatbot],\n",
    "        )\n",
    "\n",
    "        query_input.submit(\n",
    "            fn=ask_graph,\n",
    "            inputs=[query_input, chatbot],\n",
    "            outputs=[query_input, chatbot],\n",
    "        )\n",
    "\n",
    "        clear_btn.click(\n",
    "            fn=clear_conversation,\n",
    "            outputs=[query_input, chatbot],\n",
    "        )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe47fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dependencies for Enhanced GPU Mentor\n",
    "import sys\n",
    "\n",
    "required_packages = {\n",
    "    'plotly': 'plotly',\n",
    "    'gradio': 'gradio', \n",
    "    'pandas': 'pandas',\n",
    "    'numpy': 'numpy'\n",
    "}\n",
    "\n",
    "missing_packages = []\n",
    "\n",
    "for package, import_name in required_packages.items():\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"‚úÖ {package} - Available\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {package} - Missing\")\n",
    "        missing_packages.append(package)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\nInstall missing packages with:\")\n",
    "    print(f\"pip install {' '.join(missing_packages)}\")\n",
    "else:\n",
    "    print(\"\\nüéâ All required packages are available!\")\n",
    "\n",
    "# Check Sol-specific modules (these should be available when running on Sol)\n",
    "print(\"\\n--- Sol-specific checks ---\")\n",
    "sol_modules = ['subprocess', 'uuid', 'pathlib', 'json', 'tempfile']\n",
    "for module in sol_modules:\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"‚úÖ {module} - Available\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {module} - Missing (this should not happen)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591264d3",
   "metadata": {},
   "source": [
    "# GPU Mentor: Enhanced RAG with Code Execution & Benchmarking\n",
    "\n",
    "This enhanced version of the Agentic RAG system includes:\n",
    "- **Code Execution on Sol**: Submit and execute user code on Sol's GPU nodes\n",
    "- **Performance Benchmarking**: Compare CPU vs GPU performance with RAPIDS libraries\n",
    "- **Code Optimization**: Automatically suggest GPU-accelerated alternatives\n",
    "- **Interactive Learning**: Socratic questioning to guide learning\n",
    "\n",
    "## Architecture Overview\n",
    "1. **RAG Agent**: Existing system for answering questions about GPU acceleration\n",
    "2. **Code Executor**: Submits jobs to Sol via SLURM\n",
    "3. **Benchmark Engine**: Measures and compares CPU/GPU performance\n",
    "4. **Code Optimizer**: Suggests RAPIDS/CuPy alternatives\n",
    "5. **Enhanced UI**: Comprehensive interface for code playground and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40018daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced imports for GPU Mentor\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import ast\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6dca4",
   "metadata": {},
   "source": [
    "## 11. Sol Code Executor - SLURM Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b85430",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolCodeExecutor:\n",
    "    \"\"\"\n",
    "    Executes code on Sol supercomputer via SLURM job submission.\n",
    "    Handles both CPU and GPU benchmarking jobs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_work_dir=\"/tmp/gpu_mentor\"):\n",
    "        self.base_work_dir = Path(base_work_dir)\n",
    "        self.base_work_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def create_slurm_script(self, code: str, job_type: str = \"cpu\", \n",
    "                           time_limit: str = \"00:15:00\", \n",
    "                           memory: str = \"32G\") -> str:\n",
    "        \"\"\"Create SLURM batch script for code execution.\"\"\"\n",
    "        \n",
    "        job_id = str(uuid.uuid4())[:8]\n",
    "        script_content = \"\"\n",
    "        \n",
    "        if job_type == \"cpu\":\n",
    "            script_content = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=gpu_mentor_cpu_{job_id}\n",
    "#SBATCH --partition=general\n",
    "#SBATCH --qos=public\n",
    "#SBATCH --time={time_limit}\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --mem={memory}\n",
    "#SBATCH --output=cpu_output_{job_id}.out\n",
    "#SBATCH --error=cpu_error_{job_id}.err\n",
    "\n",
    "# Load necessary modules\n",
    "module load python/3.11\n",
    "module load anaconda3\n",
    "\n",
    "# Activate conda environment with CPU libraries\n",
    "source activate base\n",
    "\n",
    "# Create timing wrapper\n",
    "cat > benchmark_cpu_{job_id}.py << 'SCRIPT_EOF'\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "try:\n",
    "{self._indent_code(code)}\n",
    "    execution_status = \"success\"\n",
    "    error_message = \"\"\n",
    "except Exception as e:\n",
    "    execution_status = \"error\"\n",
    "    error_message = str(e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Save benchmark results\n",
    "results = {{\n",
    "    \"execution_time\": execution_time,\n",
    "    \"job_type\": \"cpu\",\n",
    "    \"job_id\": \"{job_id}\",\n",
    "    \"status\": execution_status,\n",
    "    \"error\": error_message\n",
    "}}\n",
    "\n",
    "with open(\"cpu_benchmark_{job_id}.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(f\"CPU Execution time: {{execution_time:.4f}} seconds\")\n",
    "SCRIPT_EOF\n",
    "\n",
    "# Execute the benchmark script\n",
    "python benchmark_cpu_{job_id}.py\n",
    "\"\"\"\n",
    "        else:  # GPU job\n",
    "            script_content = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=gpu_mentor_gpu_{job_id}\n",
    "#SBATCH --partition=general\n",
    "#SBATCH --qos=public\n",
    "#SBATCH --time={time_limit}\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --mem={memory}\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --output=gpu_output_{job_id}.out\n",
    "#SBATCH --error=gpu_error_{job_id}.err\n",
    "\n",
    "# Load necessary modules\n",
    "module load python/3.11\n",
    "module load anaconda3\n",
    "module load cuda/12.1\n",
    "\n",
    "# Activate conda environment with GPU libraries\n",
    "source activate rapids-23.08\n",
    "\n",
    "# Create timing wrapper\n",
    "cat > benchmark_gpu_{job_id}.py << 'SCRIPT_EOF'\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "\n",
    "# Import GPU libraries\n",
    "try:\n",
    "    import cupy as cp\n",
    "    import cudf\n",
    "    import cuml\n",
    "    gpu_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"GPU libraries not available: {{e}}\")\n",
    "    gpu_available = False\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "try:\n",
    "{self._indent_code(code)}\n",
    "    execution_status = \"success\"\n",
    "    error_message = \"\"\n",
    "except Exception as e:\n",
    "    execution_status = \"error\"\n",
    "    error_message = str(e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Save benchmark results\n",
    "results = {{\n",
    "    \"execution_time\": execution_time,\n",
    "    \"job_type\": \"gpu\",\n",
    "    \"job_id\": \"{job_id}\",\n",
    "    \"status\": execution_status,\n",
    "    \"error\": error_message,\n",
    "    \"gpu_available\": gpu_available\n",
    "}}\n",
    "\n",
    "with open(\"gpu_benchmark_{job_id}.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(f\"GPU Execution time: {{execution_time:.4f}} seconds\")\n",
    "SCRIPT_EOF\n",
    "\n",
    "# Execute the benchmark script\n",
    "python benchmark_gpu_{job_id}.py\n",
    "\"\"\"\n",
    "        \n",
    "        return script_content, job_id\n",
    "    \n",
    "    def _indent_code(self, code: str, indent: str = \"    \") -> str:\n",
    "        \"\"\"Add proper indentation to user code for embedding in script.\"\"\"\n",
    "        return \"\\n\".join(indent + line for line in code.split(\"\\n\"))\n",
    "    \n",
    "    def submit_job(self, script_content: str, job_id: str) -> str:\n",
    "        \"\"\"Submit job to SLURM and return job ID.\"\"\"\n",
    "        script_path = self.base_work_dir / f\"job_{job_id}.sh\"\n",
    "        \n",
    "        with open(script_path, 'w') as f:\n",
    "            f.write(script_content)\n",
    "        \n",
    "        try:\n",
    "            # Submit job via sbatch\n",
    "            result = subprocess.run(\n",
    "                [\"sbatch\", str(script_path)],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                cwd=self.base_work_dir\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                # Extract SLURM job ID from output\n",
    "                slurm_job_id = result.stdout.strip().split()[-1]\n",
    "                return slurm_job_id\n",
    "            else:\n",
    "                raise Exception(f\"Job submission failed: {result.stderr}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error submitting job: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def check_job_status(self, slurm_job_id: str) -> str:\n",
    "        \"\"\"Check the status of a SLURM job.\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"squeue\", \"-j\", slurm_job_id, \"-h\", \"-o\", \"%T\"],\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0 and result.stdout.strip():\n",
    "                return result.stdout.strip()\n",
    "            else:\n",
    "                # Job might be completed, check sacct\n",
    "                result = subprocess.run(\n",
    "                    [\"sacct\", \"-j\", slurm_job_id, \"-n\", \"-o\", \"State\"],\n",
    "                    capture_output=True,\n",
    "                    text=True\n",
    "                )\n",
    "                if result.returncode == 0 and result.stdout.strip():\n",
    "                    return result.stdout.strip().split()[0]\n",
    "                else:\n",
    "                    return \"UNKNOWN\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking job status: {e}\")\n",
    "            return \"ERROR\"\n",
    "    \n",
    "    def get_job_results(self, job_id: str, job_type: str) -> Dict:\n",
    "        \"\"\"Retrieve benchmark results from completed job.\"\"\"\n",
    "        result_file = self.base_work_dir / f\"{job_type}_benchmark_{job_id}.json\"\n",
    "        \n",
    "        if result_file.exists():\n",
    "            with open(result_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            return {\"error\": \"Results file not found\"}\n",
    "    \n",
    "    def cleanup_job_files(self, job_id: str):\n",
    "        \"\"\"Clean up temporary job files.\"\"\"\n",
    "        patterns = [\n",
    "            f\"job_{job_id}.sh\",\n",
    "            f\"*_output_{job_id}.out\",\n",
    "            f\"*_error_{job_id}.err\",\n",
    "            f\"*_benchmark_{job_id}.py\",\n",
    "            f\"*_benchmark_{job_id}.json\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            for file_path in self.base_work_dir.glob(pattern):\n",
    "                try:\n",
    "                    file_path.unlink()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error cleaning up {file_path}: {e}\")\n",
    "\n",
    "# Initialize the Sol executor\n",
    "sol_executor = SolCodeExecutor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1440d",
   "metadata": {},
   "source": [
    "## 12. Code Optimizer - GPU Acceleration Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeOptimizer:\n",
    "    \"\"\"\n",
    "    Analyzes user code and suggests GPU-accelerated alternatives using RAPIDS and CuPy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.optimization_patterns = {\n",
    "            # NumPy to CuPy optimizations\n",
    "            'numpy': {\n",
    "                'import numpy as np': 'import cupy as np',\n",
    "                'np.array(': 'cp.array(',\n",
    "                'np.random.': 'cp.random.',\n",
    "                'np.linalg.': 'cp.linalg.',\n",
    "                'np.fft.': 'cp.fft.',\n",
    "                '.cpu()': '',  # Remove .cpu() calls\n",
    "            },\n",
    "            \n",
    "            # Pandas to cuDF optimizations\n",
    "            'pandas': {\n",
    "                'import pandas as pd': 'import cudf as pd',\n",
    "                'pd.DataFrame(': 'cudf.DataFrame(',\n",
    "                'pd.Series(': 'cudf.Series(',\n",
    "                'pd.read_csv(': 'cudf.read_csv(',\n",
    "                'pd.read_parquet(': 'cudf.read_parquet(',\n",
    "                '.to_pandas()': '',  # Remove .to_pandas() calls\n",
    "            },\n",
    "            \n",
    "            # Scikit-learn to cuML optimizations\n",
    "            'sklearn': {\n",
    "                'from sklearn.': 'from cuml.',\n",
    "                'sklearn.': 'cuml.',\n",
    "            },\n",
    "            \n",
    "            # Dask optimizations\n",
    "            'dask': {\n",
    "                'import dask.array as da': 'import dask.array as da\\\\n# Configure Dask to use CuPy backend\\\\nimport dask\\\\ndask.config.set({\"array.backend\": \"cupy\"})',\n",
    "                'import dask.dataframe as dd': 'import dask_cudf as dd',\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_code(self, code: str) -> Dict[str, any]:\n",
    "        \"\"\"Analyze code for optimization opportunities.\"\"\"\n",
    "        analysis = {\n",
    "            'libraries_detected': [],\n",
    "            'optimization_opportunities': [],\n",
    "            'estimated_speedup': 1.0,\n",
    "            'gpu_compatible': True,\n",
    "            'warnings': []\n",
    "        }\n",
    "        \n",
    "        # Detect libraries used\n",
    "        for lib_type, patterns in self.optimization_patterns.items():\n",
    "            for pattern in patterns.keys():\n",
    "                if pattern in code:\n",
    "                    analysis['libraries_detected'].append(lib_type)\n",
    "                    break\n",
    "        \n",
    "        # Check for GPU incompatible operations\n",
    "        incompatible_patterns = [\n",
    "            'matplotlib.pyplot',  # Plotting might need CPU arrays\n",
    "            'pickle.dump',        # Serialization issues\n",
    "            'multiprocessing',    # GPU memory management conflicts\n",
    "        ]\n",
    "        \n",
    "        for pattern in incompatible_patterns:\n",
    "            if pattern in code:\n",
    "                analysis['warnings'].append(f\"Detected {pattern} - may require CPU data conversion\")\n",
    "        \n",
    "        # Estimate potential speedup based on operations\n",
    "        compute_intensive_ops = [\n",
    "            'np.dot', 'np.matmul', '@',  # Matrix operations\n",
    "            'np.fft', 'scipy.fft',       # FFT operations\n",
    "            '.groupby(', '.agg(',        # Aggregation operations\n",
    "            'for ' in code and 'range(' in code,  # Loops that could be vectorized\n",
    "        ]\n",
    "        \n",
    "        speedup_factors = []\n",
    "        for op in compute_intensive_ops:\n",
    "            if isinstance(op, bool):\n",
    "                if op:\n",
    "                    speedup_factors.append(5.0)  # Loop vectorization\n",
    "            elif op in code:\n",
    "                if 'matmul' in op or 'dot' in op or '@' in op:\n",
    "                    speedup_factors.append(10.0)  # Matrix ops\n",
    "                elif 'fft' in op:\n",
    "                    speedup_factors.append(15.0)  # FFT ops\n",
    "                else:\n",
    "                    speedup_factors.append(3.0)   # Other ops\n",
    "        \n",
    "        if speedup_factors:\n",
    "            analysis['estimated_speedup'] = max(speedup_factors)\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def suggest_optimizations(self, code: str) -> str:\n",
    "        \"\"\"Generate GPU-optimized version of the code.\"\"\"\n",
    "        optimized_code = code\n",
    "        \n",
    "        # Apply optimization patterns\n",
    "        for lib_type, patterns in self.optimization_patterns.items():\n",
    "            for old_pattern, new_pattern in patterns.items():\n",
    "                optimized_code = optimized_code.replace(old_pattern, new_pattern)\n",
    "        \n",
    "        # Add GPU-specific optimizations\n",
    "        if 'import cupy' in optimized_code and 'import cupy as np' not in optimized_code:\n",
    "            optimized_code = 'import cupy as cp\\\\n' + optimized_code\n",
    "        \n",
    "        # Add memory pool for better performance\n",
    "        if 'cupy' in optimized_code:\n",
    "            memory_pool_code = \"\"\"\n",
    "# Enable CuPy memory pool for better performance\n",
    "import cupy\n",
    "mempool = cupy.get_default_memory_pool()\n",
    "pinned_mempool = cupy.get_default_pinned_memory_pool()\n",
    "\"\"\"\n",
    "            optimized_code = memory_pool_code + optimized_code\n",
    "        \n",
    "        return optimized_code\n",
    "    \n",
    "    def create_benchmark_code(self, original_code: str, optimized_code: str) -> Tuple[str, str]:\n",
    "        \"\"\"Create side-by-side benchmark versions.\"\"\"\n",
    "        \n",
    "        cpu_benchmark = f\"\"\"\n",
    "# CPU Version Benchmark\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "{original_code}\n",
    "\"\"\"\n",
    "        \n",
    "        gpu_benchmark = f\"\"\"\n",
    "# GPU Version Benchmark  \n",
    "import time\n",
    "import cupy as cp\n",
    "import cudf as pd\n",
    "\n",
    "{optimized_code}\n",
    "\n",
    "# Convert final results back to CPU for comparison if needed\n",
    "# result = cp.asnumpy(result) if hasattr(result, 'get') else result\n",
    "\"\"\"\n",
    "        \n",
    "        return cpu_benchmark, gpu_benchmark\n",
    "\n",
    "# Initialize the code optimizer\n",
    "code_optimizer = CodeOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22780b6",
   "metadata": {},
   "source": [
    "## 13. Benchmark Engine - Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bda287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkEngine:\n",
    "    \"\"\"\n",
    "    Coordinates CPU vs GPU benchmarking using Sol's compute resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sol_executor: SolCodeExecutor, code_optimizer: CodeOptimizer):\n",
    "        self.sol_executor = sol_executor\n",
    "        self.code_optimizer = code_optimizer\n",
    "        self.benchmark_history = []\n",
    "    \n",
    "    def run_comprehensive_benchmark(self, user_code: str, timeout: int = 300) -> Dict:\n",
    "        \"\"\"\n",
    "        Run comprehensive CPU vs GPU benchmark.\n",
    "        \n",
    "        Args:\n",
    "            user_code: Original user code to benchmark\n",
    "            timeout: Maximum wait time for jobs to complete (seconds)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with benchmark results and visualizations\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"üîç Analyzing code for optimization opportunities...\")\n",
    "        analysis = self.code_optimizer.analyze_code(user_code)\n",
    "        \n",
    "        print(\"‚ö° Generating GPU-optimized version...\")\n",
    "        optimized_code = self.code_optimizer.suggest_optimizations(user_code)\n",
    "        \n",
    "        # Create benchmark versions\n",
    "        cpu_code, gpu_code = self.code_optimizer.create_benchmark_code(user_code, optimized_code)\n",
    "        \n",
    "        print(\"üöÄ Submitting jobs to Sol...\")\n",
    "        \n",
    "        # Submit CPU job\n",
    "        cpu_script, cpu_job_id = self.sol_executor.create_slurm_script(\n",
    "            cpu_code, job_type=\"cpu\", time_limit=\"00:15:00\"\n",
    "        )\n",
    "        cpu_slurm_id = self.sol_executor.submit_job(cpu_script, cpu_job_id)\n",
    "        \n",
    "        # Submit GPU job\n",
    "        gpu_script, gpu_job_id = self.sol_executor.create_slurm_script(\n",
    "            gpu_code, job_type=\"gpu\", time_limit=\"00:15:00\"\n",
    "        )\n",
    "        gpu_slurm_id = self.sol_executor.submit_job(gpu_script, gpu_job_id)\n",
    "        \n",
    "        if not cpu_slurm_id or not gpu_slurm_id:\n",
    "            return {\"error\": \"Failed to submit jobs to Sol\"}\n",
    "        \n",
    "        print(f\"‚úÖ Jobs submitted: CPU ({cpu_slurm_id}), GPU ({gpu_slurm_id})\")\n",
    "        print(\"‚è≥ Waiting for jobs to complete...\")\n",
    "        \n",
    "        # Wait for jobs to complete\n",
    "        start_wait = time.time()\n",
    "        cpu_status = gpu_status = \"PENDING\"\n",
    "        \n",
    "        while time.time() - start_wait < timeout:\n",
    "            cpu_status = self.sol_executor.check_job_status(cpu_slurm_id)\n",
    "            gpu_status = self.sol_executor.check_job_status(gpu_slurm_id)\n",
    "            \n",
    "            print(f\"üìä Status - CPU: {cpu_status}, GPU: {gpu_status}\")\n",
    "            \n",
    "            if cpu_status in [\"COMPLETED\", \"FAILED\"] and gpu_status in [\"COMPLETED\", \"FAILED\"]:\n",
    "                break\n",
    "                \n",
    "            time.sleep(10)  # Check every 10 seconds\n",
    "        \n",
    "        # Collect results\n",
    "        print(\"üìà Collecting benchmark results...\")\n",
    "        cpu_results = self.sol_executor.get_job_results(cpu_job_id, \"cpu\")\n",
    "        gpu_results = self.sol_executor.get_job_results(gpu_job_id, \"gpu\")\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        benchmark_results = self._process_results(\n",
    "            cpu_results, gpu_results, analysis, user_code, optimized_code\n",
    "        )\n",
    "        \n",
    "        # Store in history\n",
    "        self.benchmark_history.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"results\": benchmark_results\n",
    "        })\n",
    "        \n",
    "        # Cleanup\n",
    "        self.sol_executor.cleanup_job_files(cpu_job_id)\n",
    "        self.sol_executor.cleanup_job_files(gpu_job_id)\n",
    "        \n",
    "        return benchmark_results\n",
    "    \n",
    "    def _process_results(self, cpu_results: Dict, gpu_results: Dict, \n",
    "                        analysis: Dict, original_code: str, optimized_code: str) -> Dict:\n",
    "        \"\"\"Process and format benchmark results.\"\"\"\n",
    "        \n",
    "        results = {\n",
    "            \"analysis\": analysis,\n",
    "            \"original_code\": original_code,\n",
    "            \"optimized_code\": optimized_code,\n",
    "            \"cpu_results\": cpu_results,\n",
    "            \"gpu_results\": gpu_results,\n",
    "            \"performance_metrics\": {},\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        if (cpu_results.get(\"status\") == \"success\" and \n",
    "            gpu_results.get(\"status\") == \"success\"):\n",
    "            \n",
    "            cpu_time = cpu_results.get(\"execution_time\", 0)\n",
    "            gpu_time = gpu_results.get(\"execution_time\", 0)\n",
    "            \n",
    "            if cpu_time > 0 and gpu_time > 0:\n",
    "                speedup = cpu_time / gpu_time\n",
    "                efficiency = (speedup / analysis.get(\"estimated_speedup\", 1.0)) * 100\n",
    "                \n",
    "                results[\"performance_metrics\"] = {\n",
    "                    \"cpu_execution_time\": cpu_time,\n",
    "                    \"gpu_execution_time\": gpu_time,\n",
    "                    \"speedup_factor\": speedup,\n",
    "                    \"efficiency_percent\": efficiency,\n",
    "                    \"time_saved\": cpu_time - gpu_time,\n",
    "                    \"percent_improvement\": ((cpu_time - gpu_time) / cpu_time) * 100\n",
    "                }\n",
    "        \n",
    "        # Generate recommendations\n",
    "        results[\"recommendations\"] = self._generate_recommendations(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _generate_recommendations(self, results: Dict) -> List[str]:\n",
    "        \"\"\"Generate educational recommendations based on benchmark results.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        metrics = results.get(\"performance_metrics\", {})\n",
    "        speedup = metrics.get(\"speedup_factor\", 1.0)\n",
    "        \n",
    "        if speedup > 5:\n",
    "            recommendations.append(\"üéâ Excellent GPU acceleration! This workload benefits significantly from parallel processing.\")\n",
    "        elif speedup > 2:\n",
    "            recommendations.append(\"‚úÖ Good GPU speedup achieved. Consider optimizing memory access patterns for even better performance.\")\n",
    "        elif speedup > 1.1:\n",
    "            recommendations.append(\"üìà Modest improvement with GPU. This workload may be memory-bound or have limited parallelism.\")\n",
    "        else:\n",
    "            recommendations.append(\"‚ö†Ô∏è Limited GPU benefit. Consider if this workload has sufficient computational complexity.\")\n",
    "        \n",
    "        # Check for optimization opportunities\n",
    "        analysis = results.get(\"analysis\", {})\n",
    "        if \"numpy\" in analysis.get(\"libraries_detected\", []):\n",
    "            recommendations.append(\"üí° Consider using CuPy's memory pool for better performance with repeated operations.\")\n",
    "        \n",
    "        if \"pandas\" in analysis.get(\"libraries_detected\", []):\n",
    "            recommendations.append(\"üìä cuDF provides GPU-accelerated dataframe operations similar to pandas.\")\n",
    "        \n",
    "        if analysis.get(\"warnings\"):\n",
    "            recommendations.append(\"‚ö†Ô∏è Some operations may require CPU-GPU memory transfers. Profile memory usage.\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def create_visualization(self, benchmark_results: Dict) -> go.Figure:\n",
    "        \"\"\"Create interactive visualization of benchmark results.\"\"\"\n",
    "        \n",
    "        metrics = benchmark_results.get(\"performance_metrics\", {})\n",
    "        \n",
    "        if not metrics:\n",
    "            # Create error visualization\n",
    "            fig = go.Figure()\n",
    "            fig.add_annotation(\n",
    "                text=\"Benchmark data not available\",\n",
    "                xref=\"paper\", yref=\"paper\",\n",
    "                x=0.5, y=0.5, showarrow=False,\n",
    "                font=dict(size=20)\n",
    "            )\n",
    "            return fig\n",
    "        \n",
    "        # Create comparison chart\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Execution time comparison\n",
    "        fig.add_trace(go.Bar(\n",
    "            name='CPU',\n",
    "            x=['Execution Time'],\n",
    "            y=[metrics[\"cpu_execution_time\"]],\n",
    "            marker_color='lightcoral',\n",
    "            text=[f\"{metrics['cpu_execution_time']:.3f}s\"],\n",
    "            textposition='auto'\n",
    "        ))\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            name='GPU',\n",
    "            x=['Execution Time'],\n",
    "            y=[metrics[\"gpu_execution_time\"]],\n",
    "            marker_color='lightblue',\n",
    "            text=[f\"{metrics['gpu_execution_time']:.3f}s\"],\n",
    "            textposition='auto'\n",
    "        ))\n",
    "        \n",
    "        # Add speedup annotation\n",
    "        speedup = metrics.get(\"speedup_factor\", 1.0)\n",
    "        fig.add_annotation(\n",
    "            text=f\"üöÄ {speedup:.1f}x Speedup\",\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.7, y=0.9,\n",
    "            showarrow=False,\n",
    "            font=dict(size=16, color=\"green\"),\n",
    "            bgcolor=\"lightyellow\",\n",
    "            bordercolor=\"orange\",\n",
    "            borderwidth=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"CPU vs GPU Performance Comparison\",\n",
    "            yaxis_title=\"Execution Time (seconds)\",\n",
    "            barmode='group',\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Initialize the benchmark engine\n",
    "benchmark_engine = BenchmarkEngine(sol_executor, code_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a15023",
   "metadata": {},
   "source": [
    "## 14. Enhanced GPU Mentor Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06beda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedGPUMentor:\n",
    "    \"\"\"\n",
    "    Enhanced GPU Mentor that combines RAG capabilities with code execution and benchmarking.\n",
    "    Provides comprehensive GPU acceleration tutoring with hands-on experimentation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_graph, benchmark_engine: BenchmarkEngine, code_optimizer: CodeOptimizer):\n",
    "        self.rag_graph = rag_graph\n",
    "        self.benchmark_engine = benchmark_engine\n",
    "        self.code_optimizer = code_optimizer\n",
    "        self.conversation_history = []\n",
    "        self.benchmark_results = []\n",
    "    \n",
    "    def process_user_input(self, user_input: str, code: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Process user input and provide comprehensive response with optional code execution.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = {\n",
    "            \"text_response\": \"\",\n",
    "            \"code_analysis\": None,\n",
    "            \"benchmark_results\": None,\n",
    "            \"visualization\": None,\n",
    "            \"socratic_questions\": [],\n",
    "            \"learning_objectives\": []\n",
    "        }\n",
    "        \n",
    "        # Get RAG response for the text query\n",
    "        rag_result = self.rag_graph.invoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": user_input}]\n",
    "        })\n",
    "        response[\"text_response\"] = rag_result[\"messages\"][-1].content\n",
    "        \n",
    "        # If code is provided, analyze and benchmark it\n",
    "        if code and code.strip():\n",
    "            print(\"üîç Analyzing provided code...\")\n",
    "            \n",
    "            # Analyze code for optimization opportunities\n",
    "            analysis = self.code_optimizer.analyze_code(code)\n",
    "            response[\"code_analysis\"] = analysis\n",
    "            \n",
    "            # Generate optimized version\n",
    "            optimized_code = self.code_optimizer.suggest_optimizations(code)\n",
    "            \n",
    "            # Check if user wants to run benchmark\n",
    "            benchmark_keywords = [\"benchmark\", \"compare\", \"test\", \"performance\", \"speed\", \"faster\"]\n",
    "            if any(keyword in user_input.lower() for keyword in benchmark_keywords):\n",
    "                print(\"üöÄ Running comprehensive benchmark...\")\n",
    "                \n",
    "                try:\n",
    "                    benchmark_results = self.benchmark_engine.run_comprehensive_benchmark(code)\n",
    "                    response[\"benchmark_results\"] = benchmark_results\n",
    "                    \n",
    "                    # Create visualization\n",
    "                    if benchmark_results.get(\"performance_metrics\"):\n",
    "                        response[\"visualization\"] = self.benchmark_engine.create_visualization(benchmark_results)\n",
    "                    \n",
    "                    # Store results\n",
    "                    self.benchmark_results.append(benchmark_results)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    response[\"benchmark_results\"] = {\"error\": f\"Benchmark failed: {str(e)}\"}\n",
    "            \n",
    "            # Generate Socratic questions for learning\n",
    "            response[\"socratic_questions\"] = self._generate_socratic_questions(analysis, user_input)\n",
    "            response[\"learning_objectives\"] = self._generate_learning_objectives(analysis)\n",
    "        \n",
    "        # Store conversation\n",
    "        self.conversation_history.append({\n",
    "            \"user_input\": user_input,\n",
    "            \"code\": code,\n",
    "            \"response\": response,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _generate_socratic_questions(self, analysis: Dict, user_context: str) -> List[str]:\n",
    "        \"\"\"Generate Socratic questions to guide learning based on code analysis.\"\"\"\n",
    "        questions = []\n",
    "        \n",
    "        libraries = analysis.get(\"libraries_detected\", [])\n",
    "        estimated_speedup = analysis.get(\"estimated_speedup\", 1.0)\n",
    "        \n",
    "        if \"numpy\" in libraries:\n",
    "            questions.extend([\n",
    "                \"What types of NumPy operations do you think benefit most from GPU acceleration?\",\n",
    "                \"How might memory layout (row-major vs column-major) affect GPU performance?\",\n",
    "                \"When would you choose CuPy over NumPy for a specific computation?\"\n",
    "            ])\n",
    "        \n",
    "        if \"pandas\" in libraries:\n",
    "            questions.extend([\n",
    "                \"Which pandas operations are most computationally expensive in your code?\",\n",
    "                \"How does cuDF handle string operations compared to pandas?\",\n",
    "                \"What considerations should you make when transferring data between CPU and GPU?\"\n",
    "            ])\n",
    "        \n",
    "        if estimated_speedup > 5:\n",
    "            questions.append(\"Your code has high parallelization potential. What characteristics make it suitable for GPU acceleration?\")\n",
    "        elif estimated_speedup < 2:\n",
    "            questions.append(\"This code may not benefit much from GPU acceleration. Can you identify why?\")\n",
    "        \n",
    "        # Context-specific questions\n",
    "        if \"loop\" in user_context.lower():\n",
    "            questions.append(\"How could you vectorize this loop to take advantage of GPU parallel processing?\")\n",
    "        \n",
    "        if \"machine learning\" in user_context.lower() or \"ml\" in user_context.lower():\n",
    "            questions.append(\"How do GPU memory patterns differ between training and inference workloads?\")\n",
    "        \n",
    "        return questions[:3]  # Limit to 3 questions to avoid overwhelming\n",
    "    \n",
    "    def _generate_learning_objectives(self, analysis: Dict) -> List[str]:\n",
    "        \"\"\"Generate specific learning objectives based on the code analysis.\"\"\"\n",
    "        objectives = []\n",
    "        \n",
    "        libraries = analysis.get(\"libraries_detected\", [])\n",
    "        \n",
    "        if \"numpy\" in libraries:\n",
    "            objectives.extend([\n",
    "                \"Understand when to use CuPy vs NumPy\",\n",
    "                \"Learn about GPU memory management with CuPy\",\n",
    "                \"Master array broadcasting on GPU\"\n",
    "            ])\n",
    "        \n",
    "        if \"pandas\" in libraries:\n",
    "            objectives.extend([\n",
    "                \"Compare cuDF vs pandas performance characteristics\",\n",
    "                \"Learn efficient data transfer patterns\",\n",
    "                \"Understand GPU-accelerated groupby operations\"\n",
    "            ])\n",
    "        \n",
    "        if \"sklearn\" in libraries:\n",
    "            objectives.extend([\n",
    "                \"Explore cuML algorithms and their CPU equivalents\",\n",
    "                \"Understand distributed GPU machine learning\",\n",
    "                \"Learn about GPU memory requirements for ML models\"\n",
    "            ])\n",
    "        \n",
    "        return objectives\n",
    "    \n",
    "    def generate_tutorial_content(self, topic: str) -> str:\n",
    "        \"\"\"Generate comprehensive tutorial content on specific GPU acceleration topics.\"\"\"\n",
    "        \n",
    "        tutorial_prompt = f\"\"\"\n",
    "        Create a comprehensive tutorial on {topic} for GPU acceleration. Include:\n",
    "        1. Conceptual explanation\n",
    "        2. Code examples comparing CPU vs GPU approaches\n",
    "        3. Performance considerations\n",
    "        4. Best practices\n",
    "        5. Common pitfalls to avoid\n",
    "        \n",
    "        Focus on practical, hands-on learning with RAPIDS and CuPy libraries.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.rag_graph.invoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": tutorial_prompt}]\n",
    "        })\n",
    "        \n",
    "        return result[\"messages\"][-1].content\n",
    "    \n",
    "    def get_benchmark_summary(self) -> Dict:\n",
    "        \"\"\"Get summary of all benchmark results for learning analysis.\"\"\"\n",
    "        if not self.benchmark_results:\n",
    "            return {\"message\": \"No benchmarks run yet\"}\n",
    "        \n",
    "        summary = {\n",
    "            \"total_benchmarks\": len(self.benchmark_results),\n",
    "            \"average_speedup\": 0,\n",
    "            \"best_speedup\": 0,\n",
    "            \"worst_speedup\": float('inf'),\n",
    "            \"common_patterns\": [],\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "        \n",
    "        speedups = []\n",
    "        for result in self.benchmark_results:\n",
    "            metrics = result.get(\"performance_metrics\", {})\n",
    "            if metrics:\n",
    "                speedup = metrics.get(\"speedup_factor\", 1.0)\n",
    "                speedups.append(speedup)\n",
    "        \n",
    "        if speedups:\n",
    "            summary[\"average_speedup\"] = sum(speedups) / len(speedups)\n",
    "            summary[\"best_speedup\"] = max(speedups)\n",
    "            summary[\"worst_speedup\"] = min(speedups)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Initialize the enhanced GPU mentor\n",
    "gpu_mentor = EnhancedGPUMentor(graph, benchmark_engine, code_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e3557",
   "metadata": {},
   "source": [
    "## 15. Enhanced Gradio Interface - GPU Mentor Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3052fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "\n",
    "def chat_with_mentor(message, code, chat_history):\n",
    "    \"\"\"Handle chat interactions with the GPU Mentor.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Process user input through the enhanced mentor\n",
    "        response = gpu_mentor.process_user_input(message, code)\n",
    "        \n",
    "        # Format response for chat\n",
    "        formatted_response = response[\"text_response\"]\n",
    "        \n",
    "        # Add code analysis if available\n",
    "        if response[\"code_analysis\"]:\n",
    "            analysis = response[\"code_analysis\"]\n",
    "            formatted_response += f\"\\n\\n**Code Analysis:**\\n\"\n",
    "            formatted_response += f\"- Libraries detected: {', '.join(analysis['libraries_detected'])}\\n\"\n",
    "            formatted_response += f\"- Estimated speedup: {analysis['estimated_speedup']:.1f}x\\n\"\n",
    "            formatted_response += f\"- GPU compatible: {'‚úÖ' if analysis['gpu_compatible'] else '‚ùå'}\\n\"\n",
    "            \n",
    "            if analysis['warnings']:\n",
    "                formatted_response += f\"- Warnings: {'; '.join(analysis['warnings'])}\\n\"\n",
    "        \n",
    "        # Add Socratic questions\n",
    "        if response[\"socratic_questions\"]:\n",
    "            formatted_response += f\"\\n\\n**Think About This:**\\n\"\n",
    "            for i, question in enumerate(response[\"socratic_questions\"], 1):\n",
    "                formatted_response += f\"{i}. {question}\\n\"\n",
    "        \n",
    "        # Update chat history\n",
    "        if chat_history is None:\n",
    "            chat_history = []\n",
    "        \n",
    "        chat_history.append({\"role\": \"user\", \"content\": f\"{message}\\n\\n```python\\n{code}\\n```\" if code.strip() else message})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": formatted_response})\n",
    "        \n",
    "        return \"\", \"\", chat_history, response.get(\"visualization\"), response.get(\"benchmark_results\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error: {str(e)}\"\n",
    "        if chat_history is None:\n",
    "            chat_history = []\n",
    "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": error_msg})\n",
    "        return \"\", \"\", chat_history, None, None\n",
    "\n",
    "def run_benchmark_only(code):\n",
    "    \"\"\"Run benchmark on code without chat interaction.\"\"\"\n",
    "    \n",
    "    if not code.strip():\n",
    "        return \"Please provide code to benchmark.\", None, None\n",
    "    \n",
    "    try:\n",
    "        # Analyze code\n",
    "        analysis = code_optimizer.analyze_code(code)\n",
    "        \n",
    "        # Run benchmark\n",
    "        benchmark_results = benchmark_engine.run_comprehensive_benchmark(code)\n",
    "        \n",
    "        # Create visualization\n",
    "        viz = None\n",
    "        if benchmark_results.get(\"performance_metrics\"):\n",
    "            viz = benchmark_engine.create_visualization(benchmark_results)\n",
    "        \n",
    "        # Format results\n",
    "        if benchmark_results.get(\"error\"):\n",
    "            return f\"Benchmark failed: {benchmark_results['error']}\", None, None\n",
    "        \n",
    "        metrics = benchmark_results.get(\"performance_metrics\", {})\n",
    "        if metrics:\n",
    "            result_text = f\"\"\"\n",
    "**Benchmark Results:**\n",
    "- CPU Time: {metrics['cpu_execution_time']:.4f} seconds\n",
    "- GPU Time: {metrics['gpu_execution_time']:.4f} seconds  \n",
    "- Speedup: {metrics['speedup_factor']:.2f}x\n",
    "- Time Saved: {metrics['time_saved']:.4f} seconds\n",
    "- Improvement: {metrics['percent_improvement']:.1f}%\n",
    "\n",
    "**Recommendations:**\n",
    "{chr(10).join('‚Ä¢ ' + rec for rec in benchmark_results.get('recommendations', []))}\n",
    "\"\"\"\n",
    "        else:\n",
    "            result_text = \"Benchmark completed but no performance metrics available.\"\n",
    "        \n",
    "        return result_text, viz, benchmark_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error running benchmark: {str(e)}\", None, None\n",
    "\n",
    "def analyze_code_only(code):\n",
    "    \"\"\"Analyze code for optimization opportunities.\"\"\"\n",
    "    \n",
    "    if not code.strip():\n",
    "        return \"Please provide code to analyze.\", \"\"\n",
    "    \n",
    "    try:\n",
    "        analysis = code_optimizer.analyze_code(code)\n",
    "        optimized_code = code_optimizer.suggest_optimizations(code)\n",
    "        \n",
    "        analysis_text = f\"\"\"\n",
    "**Code Analysis:**\n",
    "- Libraries detected: {', '.join(analysis['libraries_detected'])}\n",
    "- Estimated speedup potential: {analysis['estimated_speedup']:.1f}x\n",
    "- GPU compatible: {'‚úÖ Yes' if analysis['gpu_compatible'] else '‚ùå No'}\n",
    "\n",
    "**Optimization Opportunities:**\n",
    "- Matrix operations: {'‚úÖ Detected' if any(op in code for op in ['np.dot', 'np.matmul', '@']) else '‚ùå None'}\n",
    "- Array operations: {'‚úÖ Detected' if 'numpy' in analysis['libraries_detected'] else '‚ùå None'}\n",
    "- DataFrame operations: {'‚úÖ Detected' if 'pandas' in analysis['libraries_detected'] else '‚ùå None'}\n",
    "\n",
    "**Warnings:**\n",
    "{chr(10).join('‚Ä¢ ' + warning for warning in analysis['warnings']) if analysis['warnings'] else '‚Ä¢ None'}\n",
    "\"\"\"\n",
    "        \n",
    "        return analysis_text, optimized_code\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing code: {str(e)}\", \"\"\n",
    "\n",
    "def get_tutorial(topic):\n",
    "    \"\"\"Generate tutorial content for specific topics.\"\"\"\n",
    "    \n",
    "    if not topic.strip():\n",
    "        return \"Please specify a topic for the tutorial.\"\n",
    "    \n",
    "    try:\n",
    "        tutorial_content = gpu_mentor.generate_tutorial_content(topic)\n",
    "        return tutorial_content\n",
    "    except Exception as e:\n",
    "        return f\"Error generating tutorial: {str(e)}\"\n",
    "\n",
    "def clear_chat():\n",
    "    \"\"\"Clear chat history.\"\"\"\n",
    "    return None, None, None\n",
    "\n",
    "# Sample code examples for quick testing\n",
    "sample_codes = {\n",
    "    \"Matrix Multiplication\": '''import numpy as np\n",
    "\n",
    "# Create large matrices\n",
    "n = 2000\n",
    "A = np.random.rand(n, n)\n",
    "B = np.random.rand(n, n)\n",
    "\n",
    "# Matrix multiplication\n",
    "C = np.dot(A, B)\n",
    "print(f\"Result shape: {C.shape}\")''',\n",
    "    \n",
    "    \"DataFrame Operations\": '''import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create large dataset\n",
    "n = 1000000\n",
    "df = pd.DataFrame({\n",
    "    'x': np.random.randn(n),\n",
    "    'y': np.random.randn(n),\n",
    "    'group': np.random.choice(['A', 'B', 'C'], n)\n",
    "})\n",
    "\n",
    "# Compute grouped statistics\n",
    "result = df.groupby('group').agg({\n",
    "    'x': ['mean', 'std'],\n",
    "    'y': ['sum', 'count']\n",
    "})\n",
    "print(result)''',\n",
    "    \n",
    "    \"FFT Operations\": '''import numpy as np\n",
    "\n",
    "# Generate signal\n",
    "n = 1000000\n",
    "t = np.linspace(0, 1, n)\n",
    "signal = np.sin(2 * np.pi * 50 * t) + np.sin(2 * np.pi * 120 * t)\n",
    "\n",
    "# Compute FFT\n",
    "fft_result = np.fft.fft(signal)\n",
    "frequencies = np.fft.fftfreq(n)\n",
    "print(f\"FFT computed for {n} points\")'''\n",
    "}\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks(title=\"GPU Mentor - AI Tutor for GPU Acceleration\", theme=gr.themes.Soft()) as demo:\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    # üöÄ GPU Mentor: AI Tutor for GPU Acceleration\n",
    "    \n",
    "    Learn GPU acceleration with hands-on experimentation! This AI tutor helps you:\n",
    "    - **Understand** GPU acceleration concepts through conversation\n",
    "    - **Experiment** with your code in a safe playground \n",
    "    - **Benchmark** CPU vs GPU performance on Sol supercomputer\n",
    "    - **Learn** through Socratic questioning and personalized tutorials\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        \n",
    "        # Main Chat & Code Playground Tab\n",
    "        with gr.Tab(\"üí¨ Chat & Code Playground\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"### Ask Questions & Submit Code\")\n",
    "                    \n",
    "                    message_input = gr.Textbox(\n",
    "                        label=\"Your Question or Comment\",\n",
    "                        placeholder=\"Ask about GPU acceleration, RAPIDS, CuPy, or describe what you want to learn...\",\n",
    "                        lines=2\n",
    "                    )\n",
    "                    \n",
    "                    code_input = gr.Code(\n",
    "                        label=\"Your Python Code (Optional)\",\n",
    "                        language=\"python\",\n",
    "                        lines=10\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        submit_btn = gr.Button(\"üöÄ Submit\", variant=\"primary\")\n",
    "                        clear_btn = gr.Button(\"üßπ Clear Chat\")\n",
    "                    \n",
    "                    # Sample code selector\n",
    "                    gr.Markdown(\"### Quick Examples\")\n",
    "                    sample_dropdown = gr.Dropdown(\n",
    "                        choices=list(sample_codes.keys()),\n",
    "                        label=\"Load Sample Code\",\n",
    "                        value=None\n",
    "                    )\n",
    "                \n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"### AI Mentor Response\")\n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"Conversation\",\n",
    "                        height=400,\n",
    "                        type=\"messages\"\n",
    "                    )\n",
    "                    \n",
    "                    # Benchmark visualization\n",
    "                    benchmark_plot = gr.Plot(label=\"Performance Comparison\")\n",
    "        \n",
    "        # Dedicated Benchmark Tab\n",
    "        with gr.Tab(\"üìä Performance Benchmarking\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    benchmark_code = gr.Code(\n",
    "                        label=\"Code to Benchmark\",\n",
    "                        language=\"python\", \n",
    "                        lines=15\n",
    "                    )\n",
    "                    \n",
    "                    run_benchmark_btn = gr.Button(\"‚ö° Run Benchmark on Sol\", variant=\"primary\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    benchmark_results = gr.Textbox(\n",
    "                        label=\"Benchmark Results\", \n",
    "                        lines=15\n",
    "                    )\n",
    "                    \n",
    "                    benchmark_viz = gr.Plot(label=\"Performance Visualization\")\n",
    "        \n",
    "        # Code Analysis Tab\n",
    "        with gr.Tab(\"üîç Code Analysis & Optimization\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    analyze_code = gr.Code(\n",
    "                        label=\"Code to Analyze\",\n",
    "                        language=\"python\",\n",
    "                        lines=15\n",
    "                    )\n",
    "                    \n",
    "                    analyze_btn = gr.Button(\"üîç Analyze Code\", variant=\"primary\")\n",
    "                    \n",
    "                    analysis_results = gr.Textbox(\n",
    "                        label=\"Analysis Results\",\n",
    "                        lines=10\n",
    "                    )\n",
    "                \n",
    "                with gr.Column():\n",
    "                    optimized_code = gr.Code(\n",
    "                        label=\"GPU-Optimized Version\",\n",
    "                        language=\"python\",\n",
    "                        lines=20\n",
    "                    )\n",
    "        \n",
    "        # Tutorial Generator Tab  \n",
    "        with gr.Tab(\"üìö Personalized Tutorials\"):\n",
    "            with gr.Column():\n",
    "                tutorial_topic = gr.Textbox(\n",
    "                    label=\"Tutorial Topic\",\n",
    "                    placeholder=\"e.g., 'CuPy memory management', 'cuDF vs pandas performance', 'RAPIDS machine learning'...\",\n",
    "                    lines=1\n",
    "                )\n",
    "                \n",
    "                generate_tutorial_btn = gr.Button(\"üìù Generate Tutorial\", variant=\"primary\")\n",
    "                \n",
    "                tutorial_content = gr.Markdown(\n",
    "                    label=\"Tutorial Content\",\n",
    "                    value=\"Enter a topic above to generate a personalized tutorial.\"\n",
    "                )\n",
    "        \n",
    "        # Learning Progress Tab\n",
    "        with gr.Tab(\"üìà Learning Progress\"):\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### Your GPU Acceleration Learning Journey\")\n",
    "                \n",
    "                progress_btn = gr.Button(\"üìä View Progress Summary\")\n",
    "                progress_summary = gr.JSON(label=\"Learning Summary\")\n",
    "    \n",
    "    # Event handlers\n",
    "    def load_sample_code(sample_name):\n",
    "        if sample_name and sample_name in sample_codes:\n",
    "            return sample_codes[sample_name]\n",
    "        return \"\"\n",
    "    \n",
    "    # Wire up the interface\n",
    "    sample_dropdown.change(load_sample_code, inputs=[sample_dropdown], outputs=[code_input])\n",
    "    \n",
    "    submit_btn.click(\n",
    "        chat_with_mentor,\n",
    "        inputs=[message_input, code_input, chatbot],\n",
    "        outputs=[message_input, code_input, chatbot, benchmark_plot, gr.State()]\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(clear_chat, outputs=[chatbot, benchmark_plot, gr.State()])\n",
    "    \n",
    "    run_benchmark_btn.click(\n",
    "        run_benchmark_only,\n",
    "        inputs=[benchmark_code],\n",
    "        outputs=[benchmark_results, benchmark_viz, gr.State()]\n",
    "    )\n",
    "    \n",
    "    analyze_btn.click(\n",
    "        analyze_code_only,\n",
    "        inputs=[analyze_code],\n",
    "        outputs=[analysis_results, optimized_code]\n",
    "    )\n",
    "    \n",
    "    generate_tutorial_btn.click(\n",
    "        get_tutorial,\n",
    "        inputs=[tutorial_topic],\n",
    "        outputs=[tutorial_content]\n",
    "    )\n",
    "    \n",
    "    progress_btn.click(\n",
    "        lambda: gpu_mentor.get_benchmark_summary(),\n",
    "        outputs=[progress_summary]\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        share=True,\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7860,\n",
    "        show_error=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e095a615",
   "metadata": {},
   "source": [
    "## 16. Example Usage & Testing\n",
    "\n",
    "Let's test the GPU Mentor system with some example interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dcd7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the code optimizer\n",
    "sample_numpy_code = \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# Create large matrices\n",
    "n = 1000\n",
    "A = np.random.rand(n, n)\n",
    "B = np.random.rand(n, n)\n",
    "\n",
    "# Matrix multiplication\n",
    "C = np.dot(A, B)\n",
    "print(f\"Result shape: {C.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Testing Code Optimizer ===\")\n",
    "analysis = code_optimizer.analyze_code(sample_numpy_code)\n",
    "print(\"Analysis:\", analysis)\n",
    "\n",
    "optimized = code_optimizer.suggest_optimizations(sample_numpy_code)\n",
    "print(\"\\nOptimized code:\")\n",
    "print(optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39546133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced GPU Mentor (without actual Sol execution for demo)\n",
    "print(\"\\n=== Testing Enhanced GPU Mentor ===\")\n",
    "\n",
    "# Simulate a user interaction\n",
    "user_question = \"How can I accelerate matrix multiplication with CuPy?\"\n",
    "sample_code = \"\"\"\n",
    "import numpy as np\n",
    "A = np.random.rand(500, 500)\n",
    "B = np.random.rand(500, 500)\n",
    "C = np.dot(A, B)\n",
    "\"\"\"\n",
    "\n",
    "# Test just the RAG response and code analysis (skip actual benchmarking)\n",
    "try:\n",
    "    # Get RAG response\n",
    "    rag_result = gpu_mentor.rag_graph.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": user_question}]\n",
    "    })\n",
    "    print(\"RAG Response:\", rag_result[\"messages\"][-1].content[:200] + \"...\")\n",
    "    \n",
    "    # Analyze code\n",
    "    analysis = gpu_mentor.code_optimizer.analyze_code(sample_code)\n",
    "    print(\"\\nCode Analysis:\", analysis)\n",
    "    \n",
    "    # Generate Socratic questions\n",
    "    questions = gpu_mentor._generate_socratic_questions(analysis, user_question)\n",
    "    print(\"\\nSocratic Questions:\")\n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f\"{i}. {q}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error testing GPU Mentor: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.06",
   "language": "python",
   "name": "genai25.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
