# Data Den
Data Den is a comprehensive AI-powered workspace designed to help users learn, implement, and benchmark GPU acceleration techniques for data science applications. Featuring a multi-modal interface with a conversational RAG-based AI agent, Data Den offers a highly personalized experience for both beginners and advanced users. Hosted as a standalone Python app on Sol, it enables users to optimize and run custom tasks on GPUs through sbatch scripting. With seamless code optimization, benchmarking, guided analysis, and transparent workflow explanations, Data Den is poised for integration into classrooms, research labs, and collaborative environments, advancing accessible and high-performance data science.

## 🏗️ System Architecture Overview

![image](https://github.com/user-attachments/assets/f8a921f4-6751-4a1a-8464-05bf06a55768)

### **Components:**

1. **Agentic RAG Foundation** (Sections 1-10)
   - LangGraph-based conversation flow with document retrieval
   - Local Ollama LLM integration (qwen3:14b, qwen2.5-coder:14b models)
   - Document grading and query rewriting capabilities
   - Basic Gradio chat interface
   
2. **Enhanced GPU Mentor Agent** (Section 14)
   - Integration layer combining all components
   - Socratic questioning generation
   - Learning objective tracking
   - Conversation history management

2. **Sol Code Executor** (Section 11)
   - SLURM batch job submission system
   - Automated CPU/GPU environment setup
   - Job status monitoring and result collection
   - Secure file management and cleanup

3. **Code Optimizer** (Section 12)
   - Pattern-based optimization detection (NumPy→CuPy, Pandas→cuDF, sklearn→cuML)
   - GPU compatibility analysis
   - Automatic code transformation suggestions
   - Performance potential estimation

4. **Benchmark Engine** (Section 13)
   - Coordinated CPU vs GPU performance comparison
   - Real-time job monitoring on Sol
   - Interactive visualization generation (Plotly)
   - Educational recommendation system

6. **Advanced Gradio Interface** (Section 15)
   - Multi-tab interface (Chat, Benchmarking, Analysis, Tutorials, Progress)
   - Real-time code analysis and visualization
   - Sample code library integration
   - Progress tracking and learning analytics

7. **Comprehensive Sample Code Library** (Section 17)
   - 7 categories of test codes (DataFrame, ML, Numerical, Image, Graph, Monte Carlo, GPU examples)
   - Realistic computational workloads
   - Educational code patterns
   - Testing and validation scenarios

---

## 🔧 Tech Deck

### **Core Stack**
- **Language**: Python 3.12
- **AI Framework**: LangChain + LangGraph
- **LLM**: Ollama (qwen3:14b, qwen2.5-coder:14b)
- **GPU Libraries**: RAPIDS (cuDF, cuML), CuPy
- **HPC**: SLURM on Sol supercomputer
- **UI**: Gradio with multi-tab interface
- **Visualization**: Plotly for interactive charts
- **Development**: Jupyter Notebooks, VS Code + Github Copilot Agent


## Project Setup

**Requirements**
- **Access to ASU Sol**
- **Interactive/remote session on a GPU Node**
- **Access to Ollama LLM**: default:/data/datasets/community/ollama
- **Pre-built Environments**: genai25.06, rapids25.02

Clone or download the repository or just the /App folder

```bash
git clone https://github.com/Vihaan004/R1
cd App
```

Setting up the Sol Environment for the app:

```bash
# Load the latest Mamba module for environment management
module load mamba/latest

# Activate the pre-configured Conda environment for GenAI
source activate genai25.06

# Load the Ollama module (version 0.9.0) for local LLM serving
module load ollama/0.9.0

# Set the path to the directory containing Ollama models
export OLLAMA_MODELS=/data/datasets/community/ollama

# Start the Ollama server for LLM inference
ollama-start
```
Install Python dependencies:

```bash
pip install -r requirements.txt
```

Launch the app 

```bash
python launch.py
```

Click on the live link generated by the script to access the temporarily hosted app.
---


