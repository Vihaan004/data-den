#!/bin/bash
# Check and start Ollama service for GPU Mentor

echo "ğŸ” Checking Ollama service status..."

# Check if Ollama is running
if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "âŒ Ollama service not running"
    echo "ğŸš€ Starting Ollama service..."
    
    # Start Ollama in background
    ollama serve &
    OLLAMA_PID=$!
    
    # Wait for service to start
    echo "â³ Waiting for Ollama to start..."
    for i in {1..30}; do
        if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
            echo "âœ… Ollama service started successfully"
            break
        fi
        sleep 1
        echo -n "."
    done
    
    if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "âŒ Failed to start Ollama service"
        exit 1
    fi
else
    echo "âœ… Ollama service is already running"
fi

# Check if required model is available
echo ""
echo "ğŸ” Checking for required model..."
if ollama list | grep -q "qwen2:14b"; then
    echo "âœ… Model qwen2:14b is available"
else
    echo "âŒ Model qwen2:14b not found"
    echo "ğŸ“¥ Pulling model (this may take several minutes)..."
    ollama pull qwen2:14b
    
    if [ $? -eq 0 ]; then
        echo "âœ… Model qwen2:14b downloaded successfully"
    else
        echo "âŒ Failed to download model qwen2:14b"
        exit 1
    fi
fi

echo ""
echo "ğŸ‰ Ollama is ready for GPU Mentor!"
echo "ğŸš€ You can now run: python gradio_ui.py"
