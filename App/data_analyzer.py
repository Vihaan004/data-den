import os
import glob
import pandas as pd
import gradio as gr
import base64
import io
from PIL import Image
from job_executor import run_data_analysis

class DataAnalyzer:
    """Component for analyzing datasets with GPU-accelerated operations."""
    
    def __init__(self, rag_agent):
        self.rag_agent = rag_agent
        self.current_dataset = None
        self.current_dataset_path = None
        self.dataset_info = None
        
    def get_available_datasets(self):
        """Get list of available CSV datasets from the datasets folder."""
        datasets_dir = "./datasets"
        if not os.path.exists(datasets_dir):
            return []
        
        csv_files = glob.glob(os.path.join(datasets_dir, "*.csv"))
        return [os.path.basename(f) for f in csv_files]
    
    def load_dataset(self, dataset_file, uploaded_file):
        """Load a dataset from file selection or upload."""
        try:
            if uploaded_file is not None:
                # Use uploaded file
                df = pd.read_csv(uploaded_file.name)
                self.current_dataset_path = uploaded_file.name
                dataset_name = os.path.basename(uploaded_file.name)
            elif dataset_file and dataset_file != "None":
                # Use selected sample dataset
                dataset_path = os.path.join("./datasets", dataset_file)
                df = pd.read_csv(dataset_path)
                self.current_dataset_path = dataset_path
                dataset_name = dataset_file
            else:
                return "‚ùå Please select a dataset or upload a file."
            
            self.current_dataset = df
            
            # Store dataset info for LLM
            self.dataset_info = {
                "name": dataset_name,
                "shape": df.shape,
                "columns": df.columns.tolist(),
                "dtypes": df.dtypes.to_dict(),
                "sample_data": df.head(5).to_dict(),
                "missing_values": df.isnull().sum().to_dict(),
                "numeric_columns": df.select_dtypes(include=['number']).columns.tolist(),
                "categorical_columns": df.select_dtypes(include=['object']).columns.tolist()
            }
            
            return f"‚úÖ Dataset '{dataset_name}' loaded successfully! Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns. You can now generate analysis suggestions."
            
        except Exception as e:
            return f"‚ùå Error loading dataset: {str(e)}"
    
    def generate_suggestions(self):
        """Generate analysis suggestions using the RAG agent."""
        if self.current_dataset is None or self.dataset_info is None:
            return "‚ùå Please load a dataset first."
        
        try:
            # Prepare simple prompt for LLM
            prompt = f"""Based on this dataset, suggest exactly 5 specific data analysis tasks that would benefit from GPU acceleration:

Dataset: {self.dataset_info['name']}
Columns: {', '.join(self.dataset_info['columns'])}
Numeric columns: {', '.join(self.dataset_info['numeric_columns'])}
Categorical columns: {', '.join(self.dataset_info['categorical_columns'])}
Sample data: {pd.DataFrame(self.dataset_info['sample_data']).to_string()}

Provide ONLY a numbered list of 5 simple analysis tasks that could be performed on this dataset to gain insights, with clear, specific titles. No explanations, no descriptions, just the analysis names. For example:
1. K-Means Customer Segmentation
2. Sales Correlation Analysis
3. Employee Performance Prediction
4. Salary Distribution Analysis
5. Department Efficiency Clustering"""

            # Try direct LLM call first, bypass RAG if it's causing issues
            print(f"DEBUG: Sending simplified prompt to RAG agent, length: {len(prompt)}")
            try:
                # Try to use the chat model directly for more reliable responses
                if hasattr(self.rag_agent, 'chat_llm_model') and self.rag_agent.chat_llm_model:
                    from langchain_core.messages import HumanMessage
                    response = self.rag_agent.chat_llm_model.invoke([HumanMessage(content=prompt)])
                    response = response.content if hasattr(response, 'content') else str(response)
                else:
                    response = self.rag_agent.query(prompt)
            except Exception as e:
                print(f"DEBUG: LLM call failed: {e}")
                response = ""
            
            print(f"DEBUG: RAG agent response length: {len(response) if response else 0}")
            print(f"DEBUG: RAG agent response preview: {response[:200] if response else 'None'}...")
            
            # If still empty, provide a simple fallback
            if not response or len(response.strip()) == 0:
                print("DEBUG: Using simple fallback suggestions")
                response = self._get_simple_fallback_suggestions()
            
            return f"## üîç Suggested GPU-Accelerated Analyses\n\n{response}"
            
        except Exception as e:
            return f"‚ùå Error generating suggestions: {str(e)}"
    
    def generate_code(self, selected_suggestion):
        """Generate Python code for the selected analysis suggestion."""
        if self.current_dataset is None or self.dataset_info is None:
            return "‚ùå Please load a dataset first.", ""
        
        if not selected_suggestion.strip():
            return "‚ùå Please provide a specific analysis topic or method.", ""
        
        try:
            # Prepare detailed prompt for code generation
            dataset_summary = f"""
Dataset Name: {self.dataset_info['name']}
Shape: {self.dataset_info['shape'][0]} rows √ó {self.dataset_info['shape'][1]} columns
Columns: {', '.join(self.dataset_info['columns'])}
Numeric Columns: {', '.join(self.dataset_info['numeric_columns'])}
Categorical Columns: {', '.join(self.dataset_info['categorical_columns'])}

Sample Data:
{pd.DataFrame(self.dataset_info['sample_data']).to_string()}
"""
            
            prompt = f"""Generate a complete Python script for data analysis. The analysis task is: "{selected_suggestion}"

Dataset Information:
{dataset_summary}

SOL ENVIRONMENT SPECIFICATIONS:
- Python 3.12.9 with conda-forge
- RAPIDS 25.02 (cuDF 25.02.02, cuML 25.02.01, CuPy 13.4.1)
- CUDA 12.8 with NVIDIA A100 GPUs (80GB VRAM each)
- pandas 2.3.0, numpy 1.26.4, matplotlib 3.10.1, sklearn 1.7.0
- CRITICAL: cuDF cannot handle mixed data types in single operations
- CRITICAL: cuDF Series cannot be used directly with matplotlib
- GPU memory pool available but must be managed properly

IMPORTANT INSTRUCTIONS:
- The script will run in the directory: /home/vpatel69/R1/App/output/
- Load the dataset using: df = pd.read_csv('../datasets/{self.dataset_info['name']}')
- Import ALL necessary libraries at the top (pandas, cudf, cuml, cupy, matplotlib, numpy, sklearn, etc.)
- Write a complete analysis script with proper data loading, analysis, and visualization
- Use GPU libraries (cuDF, cuML, CuPy) when appropriate for acceleration
- Include detailed print statements showing progress and results
- Create and save visualizations using matplotlib (save to current directory with plt.savefig(), NOT plt.show())
- DO NOT use try-except blocks (error handling is done by the execution environment)
- Focus on GPU-accelerated operations that showcase performance benefits

CRITICAL SOL ENVIRONMENT CONSTRAINTS:
- cuDF CANNOT create DataFrames with mixed data types (strings + numbers in same operation)
- ALWAYS separate string/categorical columns from numeric operations
- cuDF DataFrame creation will fail with "Cannot create column with mixed types" error
- Solution: Create cuDF only with numeric columns, handle strings separately

GPU LIBRARY USAGE GUIDELINES:
- Use cudf.DataFrame for GPU DataFrames, NOT cuml.DataFrame (cuML has no DataFrame class)
- For correlations: use gdf.corr() ONLY on numeric columns, filter first: gdf.select_dtypes(include=['number']).corr()
- For machine learning: import specific algorithms like 'from cuml.linear_model import LinearRegression'
- For clustering: 'from cuml.cluster import KMeans'
- For dimensionality reduction: 'from cuml.decomposition import PCA'
- Always convert pandas to cuDF: gdf = cudf.from_pandas(df)
- IMPORTANT: cuDF operations work best with numeric data, filter out string/object columns before GPU operations

DATA HANDLING BEST PRACTICES:
- Always select numeric columns before correlations: gdf.select_dtypes(include=['number'])
- String/object columns (like names, departments) should be handled separately or converted to categorical
- Check data types before GPU operations: print(gdf.dtypes)
- Use gdf.describe(include='all') to see all column statistics

MATPLOTLIB PLOTTING WITH cuDF:
- cuDF Series/DataFrames cannot be used directly with matplotlib
- Convert to numpy for plotting: gdf['column'].to_numpy() or gdf['column'].values
- For machine learning with cuML, convert to cupy arrays: gdf['column'].to_cupy()
- Example: plt.scatter(gdf['x'].to_numpy(), gdf['y'].to_numpy())

CRITICAL DATA TYPE HANDLING RULES:
- cuML algorithms return numpy arrays or cupy arrays, NOT pandas/cuDF objects
- NEVER call .to_pandas() on machine learning results (predictions, transformed data, etc.)
- cuML LinearRegression.predict() returns numpy.ndarray - use directly: plt.plot(x, predictions)
- cuML KMeans.labels_ returns cupy array - convert with: labels.get() for numpy or cp.asnumpy(labels)
- cuML PCA.transform() returns cupy array - convert with: transformed.get() for numpy
- Only use .to_pandas() on cuDF Series/DataFrames, never on numpy arrays or cuML outputs
- Check data types before conversion: print(type(variable), variable.dtype if hasattr(variable, 'dtype') else 'no dtype')

COMMON ERRORS TO AVOID:
- ‚ùå predictions.to_pandas() when predictions is numpy.ndarray
- ‚ùå labels.to_pandas() when labels is cupy.ndarray  
- ‚ùå transformed_data.to_pandas() when transformed_data is cupy.ndarray
- ‚úÖ gdf['column'].to_pandas() when gdf['column'] is cudf.Series
- ‚úÖ predictions (direct use when it's already numpy.ndarray)
- ‚úÖ labels.get() when labels is cupy.ndarray and you need numpy
- ‚úÖ cp.asnumpy(cupy_array) to convert cupy to numpy

Generate a complete, standalone Python script:

REMEMBER: Machine learning algorithms (LinearRegression, KMeans, PCA) return numpy/cupy arrays, NOT pandas/cuDF objects. Use these outputs directly for plotting - never call .to_pandas() on them!

EXAMPLE STRUCTURE:
```python
import pandas as pd
import cudf
import matplotlib.pyplot as plt
import numpy as np
import cupy as cp
from cuml.linear_model import LinearRegression  # Import specific algorithms

# Load dataset
df = pd.read_csv('../datasets/{self.dataset_info['name']}')
print(f"Dataset loaded: {{df.shape}}")

# CRITICAL: Separate numeric and string columns for Sol environment
numeric_columns = df.select_dtypes(include=['number']).columns.tolist()
string_columns = df.select_dtypes(include=['object']).columns.tolist()
print(f"Numeric columns: {{numeric_columns}}")
print(f"String columns: {{string_columns}}")

# Convert ONLY numeric data to cuDF (mixed types will fail on Sol)
numeric_df = df[numeric_columns]
gdf = cudf.from_pandas(numeric_df)
print("Numeric data converted to cuDF format")

# Example: Machine Learning with proper data type handling
if len(numeric_columns) >= 2:
    # Prepare features and target
    X = gdf[numeric_columns[:-1]]  # All but last column as features
    y = gdf[numeric_columns[-1]]   # Last column as target
    
    # Train model
    model = LinearRegression()
    model.fit(X, y)
    
    # Make predictions - this returns numpy.ndarray, NOT cuDF!
    predictions = model.predict(X)
    print(f"Predictions type: {{type(predictions)}}")  # Will be numpy.ndarray
    
    # Plot results - use predictions directly (it's already numpy)
    plt.figure(figsize=(10, 6))
    plt.scatter(gdf[numeric_columns[0]].to_numpy(), y.to_numpy(), alpha=0.5, label='Actual')
    plt.scatter(gdf[numeric_columns[0]].to_numpy(), predictions, alpha=0.7, label='Predicted')
    # NOTE: predictions is numpy.ndarray, use directly - NO .to_pandas()!
    plt.xlabel(numeric_columns[0])
    plt.ylabel(numeric_columns[-1])
    plt.legend()
    plt.title('Linear Regression Results')
    plt.savefig('regression_results.png', dpi=150, bbox_inches='tight')
    plt.close()

print("Analysis completed successfully")
```"""
            
            # Get code from RAG agent
            code_response = self.rag_agent.query(prompt)
            
            # Clean up the response to extract just the code
            code = self._extract_code_from_response(code_response)
            
            # Generate explanation
            explanation_prompt = f"""Provide a clear, educational explanation for this GPU-accelerated data analysis code:

Analysis Topic: {selected_suggestion}
Dataset: {self.dataset_info['name']}

Explain:
1. What this analysis does and what insights it provides
2. Which GPU acceleration techniques are used and why
3. Expected performance benefits compared to CPU
4. Key libraries and functions being utilized

Keep the explanation concise but informative, suitable for someone learning about GPU acceleration in data science."""
            
            explanation = self.rag_agent.query(explanation_prompt)
            
            return code, explanation
            
        except Exception as e:
            return f"‚ùå Error generating code: {str(e)}", ""
    
    def _extract_code_from_response(self, response):
        """Extract clean Python code from LLM response and validate syntax."""
        if not response:
            return "# No code generated\nprint('Error: No code was generated')"
        
        # Remove markdown code blocks if present
        code = response
        if "```python" in response:
            # Extract content between ```python and ```
            start = response.find("```python") + 9
            end = response.find("```", start)
            if end != -1:
                code = response[start:end].strip()
            else:
                code = response[start:].strip()
        elif "```" in response:
            # Extract content between ``` blocks
            start = response.find("```") + 3
            end = response.find("```", start)
            if end != -1:
                code = response[start:end].strip()
            else:
                code = response[start:].strip()
        else:
            code = response.strip()
        
        # Basic syntax validation and cleanup
        try:
            # Try to compile the code to check for syntax errors
            compile(code, '<string>', 'exec')
            return code
        except SyntaxError as e:
            print(f"DEBUG: Syntax error in generated code: {e}")
            # Return a safe fallback code that is a complete script
            return f"""# Fallback analysis script
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Load dataset
df = pd.read_csv('../datasets/{self.dataset_info['name']}')
print(f"Dataset loaded: {{df.shape[0]}} rows, {{df.shape[1]}} columns")

# Basic dataset analysis
print("\\nDataset Info:")
print(df.info())

print("\\nDataset Columns:")
print(df.columns.tolist())

print("\\nSummary Statistics:")
numeric_cols = df.select_dtypes(include=['number']).columns
if len(numeric_cols) > 0:
    print(df[numeric_cols].describe())

# Create visualizations
if len(numeric_cols) > 0:
    # Distribution plots
    plt.figure(figsize=(12, 8))
    df[numeric_cols].hist(bins=20, alpha=0.7)
    plt.tight_layout()
    plt.savefig('data_distributions.png', dpi=150, bbox_inches='tight')
    plt.close()
    print("Data distribution plot saved as 'data_distributions.png'")
    
    # Correlation heatmap if we have multiple numeric columns
    if len(numeric_cols) > 1:
        plt.figure(figsize=(10, 8))
        correlation_matrix = df[numeric_cols].corr()
        plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto')
        plt.colorbar()
        plt.xticks(range(len(numeric_cols)), numeric_cols, rotation=45)
        plt.yticks(range(len(numeric_cols)), numeric_cols)
        plt.title('Correlation Matrix')
        plt.tight_layout()
        plt.savefig('correlation_matrix.png', dpi=150, bbox_inches='tight')
        plt.close()
        print("Correlation matrix plot saved as 'correlation_matrix.png'")

print("Fallback analysis completed successfully")"""
    
    def execute_analysis(self, code):
        """Execute the analysis code using the job executor."""
        if self.current_dataset is None or self.current_dataset_path is None:
            return "‚ùå Please load a dataset first.", []
        
        if not code.strip():
            return "‚ùå Please generate code first.", []
        
        try:
            # Clean output directory before running new analysis
            self.clean_output_directory()
            
            # Run the analysis job
            result = run_data_analysis(code, self.current_dataset_path, "./output")
            
            # Always show results with static header (success/failure determined by warnings in error file)
            execution_time_text = f"{result['execution_time']:.3f} seconds" if result.get('execution_time') else "Unknown"
            
            output_text = f"""


```
{result['output']}
```
"""
            
            # Handle plots
            plot_images = []
            if result.get("plots"):
                print(f"Processing {len(result['plots'])} plots from analysis")
                for i, plot_data in enumerate(result["plots"]):
                    try:
                        # Decode base64 plot data
                        image_data = base64.b64decode(plot_data)
                        image = Image.open(io.BytesIO(image_data))
                        plot_images.append(image)
                        print(f"Successfully processed plot {i+1}/{len(result['plots'])}")
                    except Exception as e:
                        print(f"Error processing plot {i}: {e}")
            else:
                print("No plots found in analysis results")
            
            print(f"Returning {len(plot_images)} plot images to UI")
            return output_text, plot_images if plot_images else []
                
        except Exception as e:
            return f"‚ùå Error executing analysis: {str(e)}", []
    
    def create_interface(self):
        """Create the Gradio interface for data analysis."""
        gr.Markdown("Upload datasets and perform GPU-accelerated analysis with AI-generated suggestions and tasks.")
        
        # Top Row: Dataset Selection and AI Suggestions
        with gr.Row():
            # Left Column: Dataset Selection
            with gr.Column(scale=1):
                dataset_dropdown = gr.Dropdown(
                    choices=["None"] + self.get_available_datasets(),
                    label="üìÅ Sample Datasets",
                    value="None"
                )
                dataset_upload = gr.File(
                    label="Or Upload CSV File",
                    file_types=[".csv"]
                )
                load_btn = gr.Button("üìä Load Dataset", variant="primary")
            
            # Right Column: AI Suggestions
            with gr.Column(scale=1):
                load_status = gr.Textbox(label="Status", interactive=False)
                gr.Markdown("### ü§ñ AI-Generated Analysis Suggestions")
                suggestions_btn = gr.Button("üîç Generate Suggestions", variant="secondary", size="lg")
                suggestions_output = gr.Markdown("Load a dataset and click 'Generate Suggestions' to get AI recommendations.", elem_classes=["suggestions-box"])
        
        gr.Markdown("---")
        
        # Code Generation Section
        with gr.Row():
            with gr.Column():
                selected_topic = gr.Textbox(
                    label="Custom Data Analysis Task",
                    placeholder="Enter or copy one of the suggested analysis topics above...",
                    lines=1
                )
                generate_code_btn = gr.Button("‚ö° Generate Analysis", variant="secondary")
        
        # Generated Code and Explanation
        with gr.Row():
            with gr.Column(scale=1):
                generated_code = gr.Code(
                    label="üìù Generated Python Code",
                    language="python",
                    lines=20,
                    max_lines=35
                )
            with gr.Column(scale=1):
                code_explanation = gr.Markdown(
                    value="### Analysis Overview:",
                    max_height=600,
                    elem_classes=["scrollable-markdown"]
                )
        
        # Execution Section
        with gr.Row():
            execute_btn = gr.Button("üöÄ Execute Analysis", variant="primary", size="lg")
        
        # Results Section
        with gr.Row():
            with gr.Column(scale=1):
                execution_output = gr.Textbox(
                    label="üìä Analysis Results:",
                    lines=20,
                    max_lines=35,
                    interactive=False
                )
                # execution_output = gr.Markdown("### Execute analysis to see results")
            with gr.Column(scale=1):
                plot_output = gr.Gallery(
                    label="üìà Generated Plots",
                    show_label=True,
                    elem_id="plot-gallery",
                    columns=1,
                    rows=1,
                    height=400,
                    allow_preview=True,
                    preview=True
                )
    
        
        # Event handlers
        load_btn.click(
            fn=self.load_dataset,
            inputs=[dataset_dropdown, dataset_upload],
            outputs=[load_status]
        )
        
        suggestions_btn.click(
            fn=self.generate_suggestions,
            inputs=[],
            outputs=[suggestions_output]
        )
        
        generate_code_btn.click(
            fn=self.generate_code,
            inputs=[selected_topic],
            outputs=[generated_code, code_explanation]
        )
        
        execute_btn.click(
            fn=self.execute_analysis,
            inputs=[generated_code],
            outputs=[execution_output, plot_output]
        )
    
    def _get_simple_fallback_suggestions(self):
        """Provide simple fallback suggestions when LLM fails."""
        if self.dataset_info is None:
            return "‚ùå No dataset information available."
        
        # Analyze dataset characteristics to provide relevant suggestions
        numeric_cols = len(self.dataset_info['numeric_columns'])
        categorical_cols = len(self.dataset_info['categorical_columns'])
        
        suggestions = []
        
        # Generate simple topic names based on dataset structure
        if numeric_cols >= 2:
            suggestions.append("1. K-Means Clustering Analysis")
        
        if numeric_cols >= 3:
            suggestions.append("2. Correlation Matrix Analysis")
        
        if numeric_cols >= 2:
            suggestions.append("3. Linear Regression Analysis")
        
        if numeric_cols >= 4:
            suggestions.append("4. Principal Component Analysis (PCA)")
        
        if categorical_cols >= 1 and numeric_cols >= 1:
            suggestions.append("5. Statistical Aggregations by Groups")
        
        # Ensure we always have 5 suggestions
        if len(suggestions) < 5:
            suggestions.append("5. Data Distribution Analysis")
        
        return "\n".join(suggestions[:5])  # Return exactly 5 suggestions
    
    def clean_output_directory(self):
        """Clean all PNG files from the output directory."""
        try:
            output_dir = "./output"
            if os.path.exists(output_dir):
                png_files = glob.glob(os.path.join(output_dir, "*.png"))
                for png_file in png_files:
                    try:
                        os.remove(png_file)
                        print(f"Cleaned: {os.path.basename(png_file)}")
                    except Exception as e:
                        print(f"Warning: Could not remove {png_file}: {e}")
                print(f"Cleaned {len(png_files)} PNG files from output directory")
            else:
                print("Output directory does not exist")
        except Exception as e:
            print(f"Error during cleanup: {e}")
